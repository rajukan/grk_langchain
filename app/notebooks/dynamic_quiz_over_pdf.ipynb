{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %pip install langchain\n",
    "# %pip install langchain-openai\n",
    "# %pip install langchainhub\n",
    "# %pip install pypdf\n",
    "# %pip install chromadb"
   ],
   "id": "aec1c88b349d73f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T00:11:32.658594Z",
     "start_time": "2025-10-05T00:11:30.278011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain"
   ],
   "id": "c6efae43b801ea83",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "def _set_env_from_file(var: str, file_path: str = \"openai_key.txt\"):\n",
    "    \"\"\"\n",
    "    Reads an API key from a specified file and sets it as an environment variable.\n",
    "    \"\"\"\n",
    "    if not os.environ.get(var):\n",
    "        try:\n",
    "            # The 'with open' statement ensures the file is closed automatically\n",
    "            with open(file_path, 'r') as f:\n",
    "                # Read the first line and strip any leading/trailing whitespace\n",
    "                key = f.readline().strip()\n",
    "\n",
    "            if key:\n",
    "                os.environ[var] = key\n",
    "                print(f\"Successfully loaded {var} from {file_path}\")\n",
    "            else:\n",
    "                print(f\"Warning: {file_path} is empty.\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Key file not found at {file_path}. Please create the file.\")\n",
    "\n",
    "# --- Execution ---\n",
    "# Set the environment variable OPENAI_API_KEY from the file\n",
    "_set_env_from_file('OPENAI_API_KEY')\n",
    "\n",
    "# Verify it was set (optional)\n",
    "# print(os.environ.get('OPENAI_API_KEY'))"
   ],
   "id": "5ddf7710760b4486",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T00:10:04.090434Z",
     "start_time": "2025-10-05T00:10:04.087087Z"
    }
   },
   "cell_type": "code",
   "source": "MODEL=\"llama3.2\"",
   "id": "32cbb8193001cca3",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T00:11:54.110539Z",
     "start_time": "2025-10-05T00:11:52.407324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pdf_path=\"C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\DSC630\\\\why-kmeans-research-paper.pdf\"\n",
    "loader=PyPDFLoader(pdf_path)\n",
    "pdf_docs = loader.load_and_split()\n",
    "pdf_docs"
   ],
   "id": "9bd4c0aa856c3e9c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'PDFCreator Version 0.9.3', 'creationdate': '2009-08-05T10:21:00-04:00', 'moddate': 'D:20090805102100', 'title': 'Jain_PRL_paper_Aug4', 'author': 'Abhishek', 'keywords': '', 'subject': '', 'source': 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\DSC630\\\\why-kmeans-research-paper.pdf', 'total_pages': 35, 'page': 0, 'page_label': '1'}, page_content='To appear in Pattern Recognition Letters, 2009.  \\nData Clustering: 50 Years Beyond K-Means1 \\n \\nAnil K. Jain \\nDepartment of Computer Science & Engineering \\nMichigan State University \\nEast Lansing, Michigan 48824 USA \\njain@cse.msu.edu  \\n \\n \\nAbstract:  Organizing data into sensible groupings is one of the most fundamental \\nmodes of understanding and learning. As an example, a common scheme of scientific \\nclassification puts organisms into a system of ranked taxa: domain, kingdom, phylum, \\nclass, etc.. Cluster analysis is the formal study of methods and algorithms for grouping, or \\nclustering, objects according to measured or perceived intrinsic characteristics or \\nsimilarity. Cluster analysis does not use category labels that tag objects with prior \\nidentifiers, i.e., class labels. The absence of category information distinguishes data \\nclustering (unsupervised learning) from classification or discriminant analysis \\n(supervised learning). The aim of clustering is to find structure in data and is therefore \\nexploratory in nature. Clustering has a long and rich history in a variety of scientific \\nfields. One of the most popular and simple clustering algorithms, K-means, was first \\npublished in 1955. In spite of the fact that K-means was proposed over 50 years ago and \\nthousands of clustering algorithms have been published since then, K-means is still  \\nwidely used. This speaks to the difficulty of designing a general purpose clustering \\nalgorithm and the ill-posed problem of clustering. We provide a brief overview of \\nclustering, summarize well known clustering methods, discuss the major challenges and \\nkey issues in designing clustering algorithms, and point out some of the emerging and \\nuseful research directions, including semi-supervised clustering, ensemble clustering, \\nsimultaneous feature selection during data clustering and large scale data clustering. \\n \\n \\n1.  Introduction \\n \\nAdvances in sensing and storage technology and dramatic growth in applications such as \\nInternet search, digital imaging, and video surveillance have created many high-volume, \\nhigh-dimensional data sets. It is estimated that the digital universe consumed \\napproximately 281 exabytes in 2007, and it is projected to be 10 times that size by 2011. \\n(One exabyte is ~10 \\n18 bytes or 1,000,000 terabytes) [Gantz, 2008]. Most of the data is \\nstored digitally in electronic media, thus providing huge potential for the development of \\nautomatic data analysis, classification, and retrieval techniques. In addition to the growth \\nin the amount of data, the variety of available data (text, image, and video) has also \\nincreased. Inexpensive digital and video cameras have made available huge archives of \\n                                                 \\n1 This paper is based on the King-Sun Fu Prize lecture delivered at the 19 th  International \\nConference on Pattern Recognition (ICPR), Tampa, FL, December 8, 2008.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'PDFCreator Version 0.9.3', 'creationdate': '2009-08-05T10:21:00-04:00', 'moddate': 'D:20090805102100', 'title': 'Jain_PRL_paper_Aug4', 'author': 'Abhishek', 'keywords': '', 'subject': '', 'source': 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\DSC630\\\\why-kmeans-research-paper.pdf', 'total_pages': 35, 'page': 1, 'page_label': '2'}, page_content='To appear in Pattern Recognition Letters, 2009.  \\nimages and videos. The prevalence of RFID tags or transponders due to their low cost \\nand small size has resulted in the deployment of millions of sensors that transmit data \\nregularly. E-mails, blogs, transaction data, and billions of Web pages create terabytes of \\nnew data every day. Many of these data streams are unstructured, adding to the difficulty \\nin analyzing them.  \\n \\nThe increase in both the volume and the variety of data requires advances in methodology \\nto automatically understand, process, and summarize the data. Data analysis techniques \\ncan be broadly classified into two major types [Tukey, 1977]: (i) exploratory  or \\ndescriptive, meaning that the investigator does not have pre-specified models or \\nhypotheses but wants to understand the general characteristics or structure of the high-\\ndimensional data, and (ii) confirmatory  or inferential, meaning that the investigator wants \\nto confirm the validity of a hypothesis/model or a set of assumptions given the available \\ndata. Many statistical techniques have been proposed to analyze the data, such as analysis \\nof variance, linear regression, discriminant analysis, canonical correlation analysis, \\nmultidimensional scaling, factor analysis, principal component analysis, and cluster \\nanalysis to name a few. A useful overview is given in [Tabachnick & Fidell, 2007]. \\n \\nIn pattern recognition, data analysis is concerned with predictive modeling: given some \\ntraining data, we want to predict the behavior of the unseen test data. This task is also \\nreferred to as learning . Often, a clear distinction is made between learning problems that \\nare (i) supervised (classification) or (ii) unsupervised (clustering), the first involving only \\nlabeled data  (training patterns with known category labels) while the latter involving \\nonly unlabeled data  [Duda et al., 2001]. Clustering is a more difficult and challenging \\nproblem than classification. There is a growing interest in a hybrid setting, called semi-\\nsupervised learning [Chapelle et al., 2006]; in semi-supervised classification, the labels \\nof only a small portion of the training data set are available. The unlabeled data, instead \\nof being discarded, are also used in the learning process. In semi-supervised clustering, \\ninstead of specifying the class labels, pair-wise constraints are specified, which is a \\nweake r way of encoding the prior knowledge A pair-wise must-link constraint \\ncorresponds to the requirement that two objects should be assigned the same cluster label, \\nwhereas the cluster labels of two objects participating in a cannot-link constraint should \\nbe different. Constraints can be particularly beneficial in data clustering [Lange et al., \\n2005, Basu et al., 2008], where precise definitions of underlying clusters are absent. In \\nthe search for good models, one would like to include all the available information, no \\nmatter whether it is unlabeled data, data with constraints, or labeled data. Figure 1 \\nillustrates this spectrum of different types of learning problems of interest in pattern \\nrecognition and machine learning.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'PDFCreator Version 0.9.3', 'creationdate': '2009-08-05T10:21:00-04:00', 'moddate': 'D:20090805102100', 'title': 'Jain_PRL_paper_Aug4', 'author': 'Abhishek', 'keywords': '', 'subject': '', 'source': 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\DSC630\\\\why-kmeans-research-paper.pdf', 'total_pages': 35, 'page': 2, 'page_label': '3'}, page_content='To appear in Pattern Recognition Letters, 2009.  \\n \\nFigure 1 Learning problems: dots correspond to points without any labels. Points with labels are denoted \\nby plus signs, asterisks, and crosses. In (c), the must-link and cannot-link constraints are denoted by solid \\nand dashed lines, respectively (figure taken from [Lange et al., 2005]). \\n \\n2.  Data clustering \\n \\nThe goal of data clustering, also known as cluster analysis, is to discover the natural  \\ngrouping(s) of a set of patterns, points, or objects. Webster [Merriam-Webster Online \\nDictionary, 2008] defines cluster analysis as “a statistical classification technique for \\ndiscovering whether the individuals of a population fall into different groups by making \\nquantitative comparisons of multiple characteristics.” An example of clustering is shown \\nin Figure 2. The objective is to develop an automatic algorithm that will discover the \\nnatural groupings (Figure 2 (b)) in  the unlabeled data (Figure 2 (a)).  \\n \\nAn operational definition of clustering can be stated as follows: Given a representation  of \\nn objects, find K  groups based on a measure of similarity such that the similarities \\nbetween objects in the same group are high while the similarities between objects in \\ndifferent groups are low. But, what is the notion of similarity? What is the definition of a \\ncluster? Figure 2 shows that clusters can differ in terms of their shape , size , and density . \\nThe presence of noise in the data makes the detection of the clusters even more difficult. \\nAn ideal cluster can be defined as a set of points that is compact  and isolated . In reality, a \\ncluster is a subjective entity that is in the eye of the beholder and whose significance and \\ninterpretation requires domain knowledge. But, while humans are excellent cluster \\nseekers in two and possibly three dimensions, we need automatic algorithms for high \\ndimensional data. It is this challenge along with the unknown number of clusters for the \\ngiven data that has resulted in thousands of clustering algorithms that have been \\npublished and that continue to appear.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'PDFCreator Version 0.9.3', 'creationdate': '2009-08-05T10:21:00-04:00', 'moddate': 'D:20090805102100', 'title': 'Jain_PRL_paper_Aug4', 'author': 'Abhishek', 'keywords': '', 'subject': '', 'source': 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\DSC630\\\\why-kmeans-research-paper.pdf', 'total_pages': 35, 'page': 3, 'page_label': '4'}, page_content='To appear in Pattern Recognition Letters, 2009.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n   \\n \\n \\n                       (a) Input data                                         (b) Desired clustering \\n \\n \\n \\n \\n \\n \\n2.1 Why clustering? \\n \\nCluster analysis is prevalent in any discipline that involves analysis of multivariate data. \\nA search via Google Scholar [gsc, 2009] found 1,660 entries with the words data \\nclustering that appeared  in 2007 alone. This vast literature speaks to the importance of \\nclustering in data analysis. It is difficult to exhaustively list the numerous scientific fields \\nand applications that have utilized clustering techniques as well as the thousands of \\npublished algorithms. Image segmentation, an important problem in computer vision, can \\nbe formulated as a clustering problem [Jain & Flynn, 1996, Frigui & Krishnapuram, \\n1999, Shi & Malik, 2000]. Documents can be clustered [Iwayama & Tokunaga, 1995] to \\ngenerate topical hierarchies for efficient information access [Sahami, 1998] or retrieval \\n[Bhatia & Deogun, 1998]. Clustering is also used to group customers into different types \\nfor efficient marketing [Arabie & Hubert, 1994], to group services delivery engagements \\nfor workforce management and planning [Hu et al., 2007] as well as to study genome \\ndata [Baldi & Hatfield, 2002] in biology. \\n \\nData clustering has been used for the following three main purposes.  \\n \\n• Underlying structure : to gain insight into data, generate hypotheses, detect \\nanomalies, and identify salient features. \\n• Natural classification : to identify the degree of similarity among forms or \\norganisms (phylogenetic relationship). \\n• Compression : as a method for organizing the data and summarizing it through \\ncluster prototypes.   \\n \\nFigure 2 Diversity of clusters . The seven clusters in (a) (denoted by seven different colors in 1(b)) differ \\nin shape, size, and density. Although these clusters are apparent to a data analyst, none of the available \\nclustering algorithms can detect all these clusters.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'PDFCreator Version 0.9.3', 'creationdate': '2009-08-05T10:21:00-04:00', 'moddate': 'D:20090805102100', 'title': 'Jain_PRL_paper_Aug4', 'author': 'Abhishek', 'keywords': '', 'subject': '', 'source': 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\DSC630\\\\why-kmeans-research-paper.pdf', 'total_pages': 35, 'page': 4, 'page_label': '5'}, page_content='To appear in Pattern Recognition Letters, 2009.  \\nAn example of class discovery is shown in Figure 3. Here, clustering was used to \\ndiscover subclasses in an online handwritten character recognition application [Connell & \\nJain, 2002]. Different users write the same digits in different ways, thereby increasing the \\nwithin-class variance. Clustering the training patterns from a class can discover new \\nsubclasses, called the lexemes in handwritten characters. Instead of using a single model \\nfor each character, multiple models based on the number of subclasses are used to \\nimprove the recognition accuracy.  \\n \\nGiven the large number of Web pages on the Internet, most search queries typically result \\nin an extremely large number of hits. This creates the need for search results to be \\norganized. Search engines like Clusty (www.clusty.org \\n) cluster the search results and \\npresent them in a more organized way to the user. \\n \\n \\n \\nFigure 3 Finding subclasses using data clustering. (a) and (b) show two different ways of writing the digit \\n2; (c) three different subclasses for the character ‘f’; (d) three different subclasses for the letter ‘y’. \\n \\n \\n2.2 Historical developments \\n \\nThe development of clustering methodology has been a truly interdisciplinary endeavor. \\nTaxonomists, social scientists, psychologists, biologists, statisticians, mathematicians, \\nengineers, computer scientists, medical researchers, and others who collect and process \\nreal data have all contributed to clustering methodology. According to JSTOR [jst, 2009], \\ndata clustering  first appeared in the title of a 1954 article dealing with anthropological \\ndata. Data clustering is also known as Q-analysis, typology, clumping, and taxonomy \\n[Jain & Dubes, 1988] depending on the field where it is applied. There are several books \\npublished on data clustering; classic ones are by Sokal and Sneath [Sokal & Sneath, \\n1963], Anderberg [Anderberg, 1973], Hartigan [Hartigan, 1975], Jain and Dubes [Jain & \\nDubes, 1988] and Duda et al. [Duda et al., 2001]. Clustering algorithms have also been \\n(a) (b) \\n(d) (c)'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'PDFCreator Version 0.9.3', 'creationdate': '2009-08-05T10:21:00-04:00', 'moddate': 'D:20090805102100', 'title': 'Jain_PRL_paper_Aug4', 'author': 'Abhishek', 'keywords': '', 'subject': '', 'source': 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\DSC630\\\\why-kmeans-research-paper.pdf', 'total_pages': 35, 'page': 5, 'page_label': '6'}, page_content='To appear in Pattern Recognition Letters, 2009.  \\nextensively studied in data mining (see books by Han and Kamber [Han & Kamber, \\n2000] and Tan et al. [Tan et al., 2005]) and machine learning [Bishop, 2006]).  \\n \\nClustering algorithms can be broadly divided into two groups: hierarchical  and \\npartitional. Hierarchical clustering algorithms recursively find nested clusters either in \\nagglomerative mode (starting with each data point in its own cluster and merging the \\nmost similar pair of clusters successively to form a cluster hierarchy) or in divisive (top-\\ndown) mode (starting with all the data points in one cluster and recursively dividing each \\ncluster into smaller clusters). Compared to hierarchical clustering algorithms, partitional \\nclustering algorithms find all the clusters simultaneously as a partition of the data and do \\nnot impose a hierarchical structure. Input to a hierarchical algorithm is an n x n similarity \\nmatrix, where n is the number of objects to be clustered. On the other hand, a partitional \\nalgorithm can use either an n x d pattern matrix, where n objects are embedded in a d-\\ndimensional feature space, or an n x n similarity matrix. Note that a similarity matrix can \\nbe easily derived from a pattern matrix, but ordination methods such as multi-\\ndimensional scaling (MDS) are needed to derive a pattern matrix from a similarity \\nmatrix. \\n \\nThe most well-known hierarchical algorithms are single-link and complete-link; the most \\npopular and the simplest partitional algorithm is K-means. Since partitional algorithms \\nare preferred in pattern recognition due to the nature of available data, our coverage here \\nis focused on these algorithms. K-means has a rich and diverse history as it was \\nindependently discovered in different scientific fields by Steinhaus (1956) [Steinhaus, \\n1956], Lloyd (proposed in 1957, published in 1982) [Lloyd, 1982], Ball & Hall (1965) \\n[Ball & Hall, 1965] and McQueen (1967) [MacQueen, 1967]. Even though K-means was \\nfirst proposed over 50 years ago, it is still one of the most widely used algorithms for \\nclustering. Ease of implementation, simplicity, efficiency, and empirical success are the \\nmain reasons for its popularity. Below we will first summarize the development in K-\\nmeans, and then discuss the major approaches that have been developed for data \\nclustering.  \\n \\n2.3 K-Means algorithm \\n  \\nLet } {\\nixX = , ni ,..., 1=  be the set of n d-dimensional points to be clustered into a set of \\nK clusters,   { , 1,..., } kC c k K = = . K-means algorithm finds a partition such that the \\nsquared error between the empirical mean of a cluster and the points in the cluster is \\nminimized. Let kµ be the mean of cluster kc . The squared error between kµ and the \\npoints in cluster kc is defined as \\n \\n∑\\n∈\\n−=\\nki cx\\nkik xcJ 2|| || )( µ . \\n \\nThe goal of K-means is to minimize the sum of the squared error over all K  clusters,'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'PDFCreator Version 0.9.3', 'creationdate': '2009-08-05T10:21:00-04:00', 'moddate': 'D:20090805102100', 'title': 'Jain_PRL_paper_Aug4', 'author': 'Abhishek', 'keywords': '', 'subject': '', 'source': 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\DSC630\\\\why-kmeans-research-paper.pdf', 'total_pages': 35, 'page': 6, 'page_label': '7'}, page_content='To appear in Pattern Recognition Letters, 2009.  \\n∑ ∑\\n= ∈\\n−=\\nK\\nk c x\\nki\\nki\\nxCJ\\n1\\n2|| || )( µ . \\n \\nMinimizing this objective function is known to be an NP-hard problem (even for K = 2) \\n[Drineas et al., 1999]. Thus K-means, which is a greedy algorithm, can only converge to \\na local minimum, even though recent study has shown with a large probability K-means \\ncould converge to the global optimum when clusters are well separated [Meila, 2006]. K-\\nmeans starts with an initial partition with K  clusters and assign patterns to clusters so as \\nto reduce the squared error. Since the squared error always decrease with an increase in \\nthe number of clusters K  (with J(C ) = 0 when K  = n), it can be minimized only for a fixed \\nnumber of clusters. The main steps of K-means algorithm are as follows [Jain & Dubes, \\n1988]. \\n \\n1. \\n Select an initial partition with K  clusters; repeat steps 2 and 3 until cluster \\nmembership stabilizes. \\n2.  Generate a new partition by assigning each pattern to its closest cluster center. \\n3.  Compute new cluster centers. \\n \\nFigure 4 shows an illustration of K-means algorithm on a two-dimensional dataset with \\nthree clusters.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n               (a) Input data                   (b) Seed point selection                 (c) Iteration 2 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n                                  (d) Iteration 3                          (e) Final clustering        \\n \\n                               \\n \\n \\n \\nFigure 4 Illustration of K-means algorithm. (a) Two-dimensional input data with three clusters; (b) three \\nseed points selected as cluster centers and initial assignment of the data points to clusters; (c) & (d) \\nintermediate iterations updating cluster labels and their centers; (e) final clustering obtained by K-means \\nalgorithm at convergence.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'PDFCreator Version 0.9.3', 'creationdate': '2009-08-05T10:21:00-04:00', 'moddate': 'D:20090805102100', 'title': 'Jain_PRL_paper_Aug4', 'author': 'Abhishek', 'keywords': '', 'subject': '', 'source': 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\DSC630\\\\why-kmeans-research-paper.pdf', 'total_pages': 35, 'page': 7, 'page_label': '8'}, page_content='To appear in Pattern Recognition Letters, 2009.  \\n \\nParameters of K-means \\n \\n \\nThe K-means algorithm requires three user-specified parameters: number of clusters \\nK , \\ncluster initialization, and distance metric. The most critical choice is K . While no perfect \\nmathematical criterion exists, a number of heuristics (see [Tibshirani et al., 2001] and \\ndiscussion therein) are available for choosing K . Typically, K-means is run independently \\nfor different values of K  and the partition that appears the most meaningful to the domain \\nexpert is selected. Different initializations can lead to different final clustering because \\nK-means only converges to local minima. One way to overcome the local minima is to \\nrun the K-means algorithm, for a given K, with multiple different initial partitions and \\nchoose the partition with the smallest squared error.  \\n \\nK-means is typically used with the Euclidean metric for computing the distance between \\npoints and cluster centers. As a result, K-means finds spherical or ball-shaped clusters in \\ndata. K-means with Mahalanobis distance metric has been used to detect hyper-\\nellipsoidal clusters [Mao & Jain, 1996], but this comes at the expense of higher \\ncomputational cost. A variant of K-means using the Itakura-Saito distance has been used \\nfor vector quantization in speech processing [Linde et al., 1980] and K-means with L \\n1 \\ndistance was proposed in [Kashima et al., 2008]. Banerjee et al.  [Banerjee et al., 2004] \\nexploits the family of Bregman distance for K-means. \\n \\nExtensions of K-means  \\n \\nThe basic K-means algorithm has been extended in many different ways. Some of these \\nextensions deal with additional heuristics involving the minimum cluster size and \\nmerging and splitting clusters. Two well-known variants of K-means in pattern \\nrecognition literature are ISODATA [Ball & Hall, 1965] and FORGY [Forgy, 1965]. In \\nK-means, each data point is assigned to a single cluster (called \\nhard assignment ). Fuzzy \\nc-means , proposed by Dunn [Dunn, 1973]  and later improved by Bezdek [Bezdek, \\n1981], is an extension of K-means where each data point can be a member of multiple \\nclusters with a membership value ( soft assignment ). A good overview of fuzzy set based \\nclustering is available in Backer (1978) [Backer, 1978]. Data reduction by replacing \\ngroup examples with their centriods before clustering them was used to speed up K-\\nmeans and fuzzy C-means in [Eschrich et al., 2003]. Some of the other significant \\nmodifications are summarized below. Steinbach et al. [Steinbach et al., 2000] proposed a \\nhierarchical divisive version of K-means, called bisecting K-means, that recursively \\npartitions the data into two clusters at each step. In [Pelleg & Moore, 1999], kd-tree is \\nused to efficiently identify the closest cluster centers for all the data points, a key step in \\nK-mean. Bradley et al. [Bradley et al., 1998] presented a fast scalable and single-pass \\nversion of K-means that does not require all the data to be fit in the memory at the same \\ntime. X-means [Pelleg & Moore, 2000] automatically finds K  by optimizing a criterion \\nsuch as Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC). In \\nK-medoid [Kaufman & Rousseeuw, 2005], clusters are represented using the median of \\nthe data instead of the mean. Kernel K-means  [Scholkopf et al., 1998] was proposed  to \\ndetect arbitrary shaped clusters, with an appropriate choice of the kernel similarity'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'PDFCreator Version 0.9.3', 'creationdate': '2009-08-05T10:21:00-04:00', 'moddate': 'D:20090805102100', 'title': 'Jain_PRL_paper_Aug4', 'author': 'Abhishek', 'keywords': '', 'subject': '', 'source': 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\DSC630\\\\why-kmeans-research-paper.pdf', 'total_pages': 35, 'page': 8, 'page_label': '9'}, page_content='To appear in Pattern Recognition Letters, 2009.  \\nfunction.  Note that all these extensions introduce some additional algorithmic parameters \\nthat must be specified by the user. \\n \\n2.4  Major approaches to clustering \\n \\nAs mentioned before, thousands of clustering algorithms have been proposed in the \\nliterature in many different scientific disciplines. This makes it extremely difficult to \\nreview all the published approaches. Nevertheless, clustering methods differ on the \\nchoice of the objective function, probabilistic generative models, and heuristics. We will \\nbriefly review some of the major approaches. \\n \\nClusters can be defined as high density regions in the feature space separated by low \\ndensity regions. Algorithms following this notion of clusters directly search for connected \\ndense regions in the feature space. Different algorithms use different definitions of \\nconnectedness. The Jarvis-Patrick algorithm defines the similarity between a pair of \\npoints as the number of common neighbors they share, where neighbors are the points \\npresent in a region of pre-specified radius around the point [Frank & Todeschini, 1994]. \\nEster et al. [Ester et al., 1996] proposed the DBSCAN clustering algorithm, which is \\nsimilar to the Jarvis-Patrick algorithm. It directly searches for connected dense regions in \\nthe feature space by estimating the density using the Parzen window method. The \\nperformance of the Jarvis-Patrick algorithm and DBSCAN depend on two parameters: \\nneighborhood size in terms of distance, and the minimum number of points in a \\nneighborhood for its inclusion in a cluster. In addition, a number of probabilistic models \\nhave been developed for data clustering that model the density function by a probabilistic \\nmixture model. These approaches assume that the data is generated from a mixture \\ndistribution, where each cluster is described by one or more mixture components \\n[McLachlan & Basford, 1987]. The EM algorithm [Dempster et al., 1977] is often used to \\ninfer the parameters in mixture models. Several Bayesian approaches have been \\ndeveloped to improve the mixture models for data clustering, including Latent Dirichlet \\nAllocation (LDA) [Blei et al., 2003], Pachinko Allocation model[Li & McCallum, 2006] \\nand undirected graphical model for data clustering [M. Welling & Hinton, 2005]. \\n \\nWhile the density based methods, particularly the non-parametric density based \\napproaches, are attractive because of their inherent ability to deal with arbitrary shaped \\nclusters, they have limitations in handling high-dimensional data. When the data is high-\\ndimensional, the feature space is usually sparse, making it difficult to distinguish high-\\ndensity regions from low-density regions. Subspace clustering algorithms overcome this \\nlimitation by finding clusters embedded in low-dimensional subspaces of the given high-\\ndimensional data. CLIQUE [Agrawal et al., 1998] is a scalable clustering algorithm \\ndesigned to find subspaces in the data with high-density clusters. Because it estimates the \\ndensity only in a low dimensional subspace, CLIQUE does not suffer from the problem \\nof high dimensionality. \\n \\nGraph theoretic clustering, sometimes referred to as spectral clustering, represents the \\ndata points as nodes in a weighted graph. The edges connecting the nodes are weighted \\nby their pair-wise similarity. The central idea is to partition the nodes into two subsets \\nA'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'PDFCreator Version 0.9.3', 'creationdate': '2009-08-05T10:21:00-04:00', 'moddate': 'D:20090805102100', 'title': 'Jain_PRL_paper_Aug4', 'author': 'Abhishek', 'keywords': '', 'subject': '', 'source': 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\DSC630\\\\why-kmeans-research-paper.pdf', 'total_pages': 35, 'page': 9, 'page_label': '10'}, page_content='To appear in Pattern Recognition Letters, 2009.  \\nand B such that the cut size, i.e., the sum of the weights assigned to the edges connecting \\nbetween nodes in A and B, is minimized. Initial algorithms solved this problem using the \\nminimum cut algorithm, which often results in clusters of imbalaned sizes. A cluster size \\n(number of data points in a cluster) constraint was later adopted by the Ratio cut \\nalgorithm [Hagen & Kahng, 1992]. An efficient approximate graph-cut based clustering \\nalgorithm with cluster size (volume of the cluster, or sum of edge weights within a \\ncluster) constraint, called Normalized Cut, was first proposed by Shi and Malik [Shi & \\nMalik, 2000]. Its multiclass version was proposed by Yu and Shi [Yu & Shi, 2003]. \\nMeila and Shi [Meila & Shi, 2001] presented a Markov Random Walk view of spectral \\nclustering and proposed the Modified Normalized Cut (MNCut) algorithm that can \\nhandle an arbitrary number of clusters. Another variant of spectral clustering algorithm \\nwas proposed by Ng et al. [Ng et al., 2001], where a new data representation is derived \\nfrom the normalized eigenvectors of a kernel matrix. Laplacian Eigenmap [Belkin & \\nNiyogi, 2002] is another spectral clustering method that derives the data representation \\nbased on the eigenvectors of the graph Laplacian. Hofmann and Buhmann [Hofmann & \\nBuhmann, 1997] proposed a deterministic annealing algorithm for clustering data \\nreprestented using proximity measures between the data objects. \\n \\nSeveral clustering algorithms have an information theoretic formulation. For example, the \\nminimum entropy method presented in [Roberts et al., 2001] assumes that the data is \\ngenerated using a mixture model and each cluster is modeled using a semi-parametric \\nprobability density. The parameters are estimated by maximizing the KL-divergence \\nbetween the unconditional density and the conditional density of a data points \\nconditioned over the cluster. This minimizes the overlap between the conditional and \\nunconditional densities, thereby separating the clusters from each other. In other words, \\nthis formulation results in an approach that minimizes the expected entropy of the \\npartitions over the observed data. The information bottleneck method [Tishby et al., \\n1999] was proposed as a generalization to the rate-distortion theory and adopts a lossy \\ndata compression view. In simple words, given a joint distribution over two random \\nvariables, Information Bottleneck compresses one of the variables while retaining the \\nmaximum amount of mutual information with respect to the other variable. An \\napplication of this to document clustering is shown in [Slonim & Tishby, 2000] where the \\ntwo random variables are words and documents. The words are clustered first, such that \\nthe mutual information with respect to documents is maximally retained, and using the \\nclustered words, the documents are clustered such that the mutual information between \\nclustered words and clustered documents is maximally retained. \\n \\n3.  User’s dilemma \\n \\nIn spite of the prevalence of such a large number of clustering algorithms, and their \\nsuccess in a number of different application domains, clustering remains a difficult \\nproblem. This can be attributed to the inherent vagueness in the definition of a cluster, \\nand the difficulty in defining an appropriate similarity measure and objective function. \\n \\nThe following fundamental challenges associated with clustering were highlighted in \\n[Jain & Dubes, 1988], which are relevant even to this day.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'PDFCreator Version 0.9.3', 'creationdate': '2009-08-05T10:21:00-04:00', 'moddate': 'D:20090805102100', 'title': 'Jain_PRL_paper_Aug4', 'author': 'Abhishek', 'keywords': '', 'subject': '', 'source': 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\DSC630\\\\why-kmeans-research-paper.pdf', 'total_pages': 35, 'page': 10, 'page_label': '11'}, page_content='To appear in Pattern Recognition Letters, 2009.  \\n \\n(a)  What is a cluster? \\n(b)  What features should be used? \\n(c)  Should the data be normalized? \\n(d)  Does the data contain any outliers? \\n(e)  How do we define the pair-wise similarity? \\n(f)  How many clusters are present in the data? \\n(g)  Which clustering method should be used? \\n(h)  Does the data have any clustering tendency? \\n(i)  Are the discovered clusters and partition valid? \\n \\nWe will highlight and illustrate some of these challenges below. \\n \\n3.1 Data representation  \\n \\nData representation  is one of the most important factors that influence the performance \\nof the clustering algorithm. If the representation (choice of features) is good, the clusters \\nare likely to be compact and isolated and even a simple clustering algorithm such as K-\\nmeans will find them. Unfortunately, there is no universally good representation; the \\nchoice of representation must be guided by the domain knowledge. Figure 5(a) shows a \\ndataset where K-means fails to partition it into the two “natural” clusters. The partition \\nobtained by K-means is shown by a dashed line in Figure 5(a). However, when the same \\ndata points in (a) are represented using the top two eigenvectors of the RBF similarity \\nmatrix computed from the data in Figure 5(b), they become well separated, making it \\ntrivial for K-means to cluster the data [Ng et al., 2001]. \\n  \\n \\n \\n \\n \\n \\n \\n                                   (a)                    (b)  \\nFigure 5 Importance of a good representation. (a) “Two rings” dataset where K-means fails to find the two \\n“natural” clusters; the dashed line shows the linear cluster separation boundary obtained by running K-\\nmeans with K = 2. (b) a new representation of the data in (a) based on the top 2 eigenvectors of the graph \\nLaplacian of the data, computed using an RBF kernel; K-means now can easily detect the two clusters'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'PDFCreator Version 0.9.3', 'creationdate': '2009-08-05T10:21:00-04:00', 'moddate': 'D:20090805102100', 'title': 'Jain_PRL_paper_Aug4', 'author': 'Abhishek', 'keywords': '', 'subject': '', 'source': 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\DSC630\\\\why-kmeans-research-paper.pdf', 'total_pages': 35, 'page': 11, 'page_label': '12'}, page_content='To appear in Pattern Recognition Letters, 2009.  \\n3.2 Purpose of grouping \\n \\nThe representation of the data is closely tied with the purpose of grouping. The \\nrepresentation must go hand in hand with the end goal of the user. An example dataset of \\n16 animals represented using 13 Boolean features was used in [Pampalk et al., 2003] to \\ndemonstrate how the representation affects the grouping. The animals are represented \\nusing 13 Boolean features related to their appearance and activity. When a large weight is \\nplaced on the appearance features compared to the activity features, the animals were \\nclustered into mammals vs. birds . On the other hand, a large weight on the activity \\nfeatures clustered the dataset into predators vs. non-predators . Both these partitioning \\nshown in Figure 6 are equally valid, and uncover meaningful structures in the data. It is \\nup to the user to carefully choose his representation to obtain a desired clustering. \\n \\n  \\n                 (a)                                                                                                   (b) \\nFigure 6 Different weights on features result in different partitioning of the data. Sixteen animals are \\nrepresented based on 13 Boolean features related to appearance and activity. (a) partitioning with large \\nweights assigned to the appearance based features; (b) a partitioning with large weights assigned to the \\nactivity features. The figures in (a) and (b) are excerpted from [Pampalk et al., 2003]), and are known as  \\n“heat maps” where the colors represent the density of samples at a location; the warmer the color, the larger \\nthe density. \\n \\n3.3 Number of Clusters \\n \\nAutomatically determining the number of clusters has been one of the most difficult \\nproblems in data clustering. Most methods for automatically determining the number of \\nclusters cast it into the problem of model selection. Usually, clustering algorithms are run \\nwith different values of K ; the best  value of K is then chosen based on a predefined \\ncriterion. Figueiredo and Jain [Figueiredo & Jain, 2002] used the minimum message \\nlength (MML) criteria [Wallace & Boulton, 1968, Wallace & Freeman, 1987] in'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'PDFCreator Version 0.9.3', 'creationdate': '2009-08-05T10:21:00-04:00', 'moddate': 'D:20090805102100', 'title': 'Jain_PRL_paper_Aug4', 'author': 'Abhishek', 'keywords': '', 'subject': '', 'source': 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\DSC630\\\\why-kmeans-research-paper.pdf', 'total_pages': 35, 'page': 12, 'page_label': '13'}, page_content='To appear in Pattern Recognition Letters, 2009.  \\nconjunction with the Gaussian mixture model (GMM) to estimate K . Their approach \\nstarts with a large number of clusters, and gradually merges the clusters if this leads to a \\ndecrease in the MML criterion. A related approach but using the principle of Minimum \\nDescription Length (MDL) was used in [Hansen & Yu, 2001] for selecting the number of \\nclusters. The other criteria for selecting the number of clusters are the Bayes Information \\nCriterion (BIC) and Akiake Information Criterion (AIC). Gap statistics [Tibshirani et al., \\n2001] is another commonly used approach for deciding the number of clusters. The key \\nassumption is that when dividing data into an optimal number of clusters, the resulting \\npartition is most resilient to the random perturbations. The Dirichlet Process (DP) \\n[Ferguson, 1973, Rasmussen, 2000] introduces a non-parametric prior for the number of \\nclusters. It is often used by probabilistic models to derive a posterior distribution for the \\nnumber of clusters, from which the most likely number of clusters can be computed. In \\nspite of these objective criteria, it is not easy to decide which value of K  leads to more \\nmeaningful clusters. Figure 7(a) shows a two-dimensional synthetic dataset generated \\nfrom a mixture of six Gaussian components. The true labels of the points are shown in \\nFigure 7(e). When a mixture of Gaussians is fit to the data with 2, 5 and 6 components, \\nshown in Figure 7(b)-(d), respectively, each one of them seems to be a reasonable fit.  \\n \\n \\nFigure 7 Automatic selection of number of clusters, K . (a) Input data generated from a mixture of six \\nGaussian distributions; (b)-(d) Gaussian mixture model (GMM) fit to the data with 2, 5 and 6 components, \\nrespectively; (e) true labels of the data. \\n \\n3.4 Cluster validity \\n(e) True labels, K = 6  \\n (c) GMM (K=5) \\n (d) GMM (K=6) \\n (b) GMM (K=2) (a) Input data'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'PDFCreator Version 0.9.3', 'creationdate': '2009-08-05T10:21:00-04:00', 'moddate': 'D:20090805102100', 'title': 'Jain_PRL_paper_Aug4', 'author': 'Abhishek', 'keywords': '', 'subject': '', 'source': 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\DSC630\\\\why-kmeans-research-paper.pdf', 'total_pages': 35, 'page': 13, 'page_label': '14'}, page_content='To appear in Pattern Recognition Letters, 2009.  \\n \\nClustering algorithms tend to find clusters in the data irrespective of whether or not any \\nclusters are present. Figure 8(a) shows a dataset with no \\nnatural clustering; the points \\nhere were generated uniformly in a unit square. However, when the K-means algorithm is \\nrun on this data with K = 3, three clusters are identified as shown in Figure 8(b)! Cluster \\nvalidity  refers to formal procedures that evaluate the results of cluster analysis in a \\nquantitative and objective fashion [Jain & Dubes, 1988]. In fact, even before a clustering \\nalgorithm is applied to the data, the user should determine if the data even has a \\nclustering tendency  [Smith & Jain, 1984]. \\n \\n \\n                          (a)                                                                                                  (b) \\n \\nFigure 8 Cluster validity. (a) A dataset with no “natural” clustering; (b) K-means partition with K = 3.  \\n \\nCluster validity indices can be defined based on three different criteria: \\ninternal , relative , \\nand external [Jain & Dubes, 1988]. Indices based on internal criteria  assess the fit \\nbetween the structure imposed by the clustering algorithm (clustering) and the data using \\nthe data alone. Indices based on relative criteria compare multiple structures (generated \\nby different algorithms, for example) and decide which of them is better in some sense. \\nExternal indices measure the performance by matching cluster structure to the a priori \\ninformation, namely the “true” class labels (often referred to as ground truth). Typically, \\nclustering results are evaluated using the external criterion, but if the true labels are \\navailable, why even bother with clustering? The notion of cluster stability [Lange et al., \\n2004] is appealing as an internal stability measure. Cluster stability is measured as the \\namount of variation in the clustering solution over different sub-samples drawn from the \\ninput data.  Different measures of variation can be used to obtain different stability \\nmeasures. In [Lange et al., 2004], a supervised classifier is trained on one of the \\nsubsamples of the data, by using the cluster labels obtained by clustering the subsample, \\nas the true  labels. The performance of this classifier on the testing subset(s) indicates the \\nstability of the clustering algorithm. In model based algorithms (e.g., centroid based \\nrepresentation of clusters in K-means, or Gaussian Mixture Models), the distance \\nbetween the models found for different subsamples can be used to measure the stability \\n[von Luxburg & David, 2005]. Shamir and Tishby [Shamir & Tishby, 2008] define'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'PDFCreator Version 0.9.3', 'creationdate': '2009-08-05T10:21:00-04:00', 'moddate': 'D:20090805102100', 'title': 'Jain_PRL_paper_Aug4', 'author': 'Abhishek', 'keywords': '', 'subject': '', 'source': 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\DSC630\\\\why-kmeans-research-paper.pdf', 'total_pages': 35, 'page': 14, 'page_label': '15'}, page_content='To appear in Pattern Recognition Letters, 2009.  \\nstability as the generalization ability of a clustering algorithm (in PAC-Bayesian sense). \\nThey argue that since many algorithms can be shown to be asymptotically stable, the rate  \\nat which the asymptotic stability is reached with respect to the number of samples is a \\nmore useful measure of cluster stability. Cross-validation is a widely used evaluation \\nmethod in supervised learning. It has been adapated to unsupervised learning by \\nreplacing the notaion of “prediction accuracy” with a different validity measure. For \\nexample, given the mixture models obtained from the data in one fold, the likelihood of \\nthe data in the other folds serves as an indication of the algorithm’s performance, and can \\nbe used to determine the number of clusters K . \\n \\n3.5 Comparing clustering algorithms \\n \\nDifferent clustering algorithms often result in entirely different partitions even on the \\nsame data. In Figure 9, seven different algorithms were applied to cluster the 15 two-\\ndimensional points. FORGY, ISODATA, CLUSTER, and WI SH are partitional \\nalgorithms that minimize the squared error criterion (they are variants of the basic K-\\nmeans algorithm). Of the remaining three algorithms, MST (minimum spanning tree) can \\nbe viewed as a single-link hierarchical algorithm, and JP is a nearest neighbor clustering \\nalgorithm. Note that a hierarchical algorithm can be used to generate a partition by \\nspecifying a threshold on the similarity. It is evident that none of the clustering is superior \\nto the other, but some are similar to the other.  \\n \\nAn interesting question is to identify algorithms that generate similar partitions \\nirrespective of the data. In other words, can we cluster the clustering algorithms? Jain et \\nal. [Jain et al., 2004] clustered 35 different clustering algorithms into 5 groups based on \\ntheir partitions on 12 different datasets. The similarity between the clustering algorithms \\nis measured as the averaged similarity between the partitions obtained on the 12 datasets. \\nThe similarity between a pair of partitions is measured using the Adjusted Rand Index \\n(ARI). A hierarchical clustering of the 35 clustering algorithms is shown in Figure 10(a). \\nIt is not surprising to see that the related algorithms are clustered together. For a \\nvisualization of the similarity between the algorithms, the 35 algorithms are also \\nembedded in a two-dimensional space; this is achieved by applying the Sammon’s \\nprojection algorithm [J. W. Sammon, 1969] to the 35x35 similarity matrix. Figure 10(b) \\nshows that all the CHAMELEON variations (6, 8-10) are clustered into a single cluster. \\nThis plot suggests that the clustering algorithms following the same clustering strategy \\nresult in similar clustering in spite of minor variations in the parameters or objective \\nfunctions involved. In [Meila, 2003], a different metric in the space of clusterings, termed \\nVariation of Information (VI), was proposed. It measures the similarity between two \\nclustering algorithms by the amount of information lost or gained when choosing one \\nclustering over the other.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'PDFCreator Version 0.9.3', 'creationdate': '2009-08-05T10:21:00-04:00', 'moddate': 'D:20090805102100', 'title': 'Jain_PRL_paper_Aug4', 'author': 'Abhishek', 'keywords': '', 'subject': '', 'source': 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\DSC630\\\\why-kmeans-research-paper.pdf', 'total_pages': 35, 'page': 15, 'page_label': '16'}, page_content='To appear in Pattern Recognition Letters, 2009.  \\n \\nFigure 9 Several clustering of fifteen patterns in two dimensions: (a) fifteen patterns; (b) minimum \\nspanning tree of the fifteen patterns; (c) clusters from FORGY; (d) clusters from ISODATA; (e) clusters \\nfrom WISH; (f) clusters from CLUSTER; (g) clusters from complete-link hierarchical clustering; and (h) \\nclusters from Jarvis-Patrick clustering algorithm.  (Figure reproduced from [Dubes & Jain, 1976]). \\n \\n \\n \\n \\nFigure 10 Clustering of clustering algorithms . (a) Hierarchical clustering of 35 different algorithms; (b) \\nSammon’s mapping of the 35 algorithms into a two-dimensional space, with the clusters highlighted for \\nvisualization. The algorithms in the group (4, 29, 31-35) correspond to K-means, spectral clustering, \\nGaussian mixture models, and Ward’s linkage. The algorithms in group (6, 8-10) correspond to \\nCHAMELEON algorithm with different objective functions. \\n \\nClustering algorithms can also be compared at the theoretical level based on their \\nobjective functions. In order to perform such a comparison, a distinction should be made \\nbetween a \\nclustering method  and a clustering algorithm [Jain & Dubes, 1988]. A \\nclustering method is a general strategy employed to solve a clustering problem. A \\n(a) 15 points in 2D  (b) MS T  (c) FORGY  (d) ISODATA  \\n(e) WISH  \\n (h) JP  (f) CLUSTER  (g) Complete -link'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'PDFCreator Version 0.9.3', 'creationdate': '2009-08-05T10:21:00-04:00', 'moddate': 'D:20090805102100', 'title': 'Jain_PRL_paper_Aug4', 'author': 'Abhishek', 'keywords': '', 'subject': '', 'source': 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\DSC630\\\\why-kmeans-research-paper.pdf', 'total_pages': 35, 'page': 16, 'page_label': '17'}, page_content='To appear in Pattern Recognition Letters, 2009.  \\nclustering algorithm, on the other hand, is simply an instance of a method. For instance, \\nminimizing the squared error is a clustering method, and there are many different \\nclustering algorithms, including K-means, that implement the minimum squared error \\nmethod. Some equivalence relationships even between different clustering methods have \\nbeen shown. For example, Dhillon et al. [Dhillon et al., 2004] show that spectral methods \\nand kernel K-means are equivalent; for a choice of kernel in spectral clustering, there \\nexists a kernel for which the objective functions of Kernel K-means and spectral \\nclustering are the same. The equivalence between non-negative matrix factorization for \\nclustering and kernel K-means algorithm is shown in [Ding et al., 2005]. All these \\nmethods are directly related to the analysis of eigenvectors of the similarity matrix. \\n \\nThe above discussion underscores one of the important facts about clustering; \\nthere is no \\nbest clustering algorithm . Each clustering algorithm imposes a structure on the data \\neither explicitly or implicitly. When there is a good match between the model and the \\ndata, good partitions are obtained. Since the structure of the data is not known a priori, \\none needs to try competing and diverse approaches to determine an appropriate algorithm \\nfor the clustering task at hand. This idea of no best clustering algorithm is partially \\ncaptured by the impossibility theorem [Kleinberg, 2002], which states that no single \\nclustering algorithm simultaneously satisfies a set of basic axioms of data clustering. \\n \\n3.6 Admissibility analysis of clustering algorithms \\n \\nFisher and Van Ness [Fisher & vanNess, 1971] formally analyzed clustering algorithms \\nwith the objective of comparing them and providing guidance in choosing a clustering \\nprocedure. They defined a set of \\nadmissibility criteria  for clustering algorithms that test \\nthe sensitivity of clustering algorithms with respect to the changes that do not alter the \\nessential structure of the data. A clustering is called A-admissible  if it satisfies criterion A. \\nExample criteria include convex, point and cluster proportion, cluster omission , and \\nmonotone . They are briefly described below. \\n \\n/square6 Convex: A clustering algorithm is convex-admissible if it results in a clustering \\nwhere the convex hulls of clusters do not intersect. \\n/square6 Cluster proportion:  A clustering algorithm is cluster-proportion admissible if the \\ncluster boundaries do not alter even if some of the clusters are duplicated an \\narbitrary number of times. \\n• Cluster omission:  A clustering algorithm is omission-admissible  if by removing \\none of the clusters from the data and re-running the algorithm, the clustering on \\nthe remaining K-1 clusters is identical to the one obtained on them with K \\nclusters. \\n• Monotone:  A clustering algorithm is monotone-admissible if the clustering results \\ndo not change when a monotone transformation is applied to the elements of the \\nsimilarity matrix. \\n \\nFisher and Van Ness proved that one cannot construct algorithms that satisfy certain \\nadmissibility criteria. For example, if an algorithm is monotone-admissible, it cannot be a \\nhierarchical clustering algorithm.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'PDFCreator Version 0.9.3', 'creationdate': '2009-08-05T10:21:00-04:00', 'moddate': 'D:20090805102100', 'title': 'Jain_PRL_paper_Aug4', 'author': 'Abhishek', 'keywords': '', 'subject': '', 'source': 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\DSC630\\\\why-kmeans-research-paper.pdf', 'total_pages': 35, 'page': 17, 'page_label': '18'}, page_content='To appear in Pattern Recognition Letters, 2009.  \\n \\nKleinberg [Kleinberg, 2002] addressed a similar problem, where he defined three criteria:  \\n \\n• Scale invariance: An arbitrary scaling of the similarity metric must not change the \\nclustering results. \\n• Richness:  The clustering algorithm must be able to achieve all possible partitions \\non the data. \\n• Consistency: By shrinking within-cluster distances and stretching between-cluster \\ndistances, the clustering results must not change. \\n \\nKleinberg also provides results similar to that of [Fisher & vanNess, 1971], showing that \\nit is impossible to construct an algorithm that satisfies all these properties hence the title \\nof his paper “An Impossibility Theorem for Clustering”. Further discussions in \\n[Kleinberg, 2002] reveal that a clustering algorithm can indeed be designed by relaxing \\nthe definition of \\nsatisfying  a criterion to nearly-satisfying  the criterion. While the set of \\naxioms defined here are reasonable to a large extent, they are in no way the only possible \\nset of axioms, and hence the results must be interpreted accordingly [S. Ben-David, \\n2008]. \\n \\n4 Trends in data clustering \\n \\nInformation explosion is not only creating large amounts of data but also a diverse set of \\ndata, both structured  and unstructured . Unstructured data is a collection of objects that \\ndo not follow a specific format. For example, images, text, audio, video, etc. On the other \\nhand, in structured data, there are semantic relationships within each object that are \\nimportant.  Most clustering approaches ignore the structure in the objects to be clustered \\nand use a feature vector based representation for both structured and unstructured data. \\nThe traditional view of data partitioning based on vector-based feature representation \\ndoes not always serve as an adequate framework. Examples include objects represented \\nusing sets of points [Lowe, 2004], consumer purchase records [Guha et al., 2000], data \\ncollected from questionnaires and rankings [Critchlow, 1985], social networks \\n[Wasserman & Faust, 1994], and data streams [Guha et al., 2003b]. Models and \\nalgorithms are being developed to process huge volumes of heterogeneous data. A brief \\nsummary of some of the recent trends in data clustering is presented below. \\n4.1 Clustering ensembles \\nThe success of ensemble methods for supervised learning has motivated the development \\nof ensemble methods for unsupervised learning [Fred & Jain, 2002]. The basic idea is \\nthat by taking multiple looks  at the same data, one can generate multiple partitions \\n(clustering ensemble ) of the same data. By combining the resulting partitions, it is \\npossible to obtain a good data partitioning even when the clusters are not compact and \\nwell separated. Fred and Jain used this approach by taking an ensemble of partitions \\nobtained by K-means; the ensemble was obtained by changing the value of K and using \\nrandom cluster initializations. These partitions were then combined using a co-occurrence \\nmatrix that resulted in a good separation of the clusters. An example of a clustering'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'PDFCreator Version 0.9.3', 'creationdate': '2009-08-05T10:21:00-04:00', 'moddate': 'D:20090805102100', 'title': 'Jain_PRL_paper_Aug4', 'author': 'Abhishek', 'keywords': '', 'subject': '', 'source': 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\DSC630\\\\why-kmeans-research-paper.pdf', 'total_pages': 35, 'page': 18, 'page_label': '19'}, page_content='To appear in Pattern Recognition Letters, 2009.  \\nensemble is shown in Figure 11 where a “two-spiral” dataset is used to demonstrate its \\neffectiveness. K-means is run multiple, say N, times with varying values of the number of \\nclusters K . The new similarity between a pair of points is defined as the number of times \\nthe two points co-occur in the same cluster in N runs of K-means. The final clustering is \\nobtained by clustering the data based on the new pair-wise similarity. Strehl and Ghosh \\n[Strehl & Ghosh, 2003] proposed several probabilistic models for integrating multiple \\npartitions. More recent work on cluster ensembles can be found in [Hore et al., 2009a].  \\nThere are many different ways of generating a clustering ensemble and then combining \\nthe partitions. For example, multiple data partitions can be generated by: (i) applying \\ndifferent clustering algorithms, (ii) applying the same clustering algorithm with different \\nvalues of parameters or initializations, and (iii) combining of different data \\nrepresentations (feature spaces) and clustering algorithms. The evidence accumulation \\nstep that combines the information provided by the different partitions can be viewed as \\nlearning the similarity measure among the data points. \\n \\n \\n \\n4.2 Semi-supervised clustering \\nClustering is inherently an ill-posed problem where the goal is to partition the data into \\nsome unknown number of clusters based on intrinsic information alone. The data-driven \\nnature of clustering makes it very difficult to design clustering algorithms that will \\ncorrectly find clusters in the given data. Any external or side information  available along \\nwith the n x d pattern matrix or the n x n similarity matrix can be extremely useful in \\nfinding a good partition of data. Clustering algorithms that utilize such side information \\nare said to be operating in a semi-supervised mode  [Chapelle et al., 2006]. There are two \\nopen questions: (i) how should the side information be specified? and (ii) how is it \\nobtained in practice? One of the most common methods of specifying the side \\n   \\n-\\n \\n0 5\\n-\\n \\n-\\n \\n-\\n \\n0\\n2\\n4\\n6\\n \\nFigure 11  Clustering ensembles. Multiple runs of K-means are used to learn the pair-wise similarity \\nusing the “co-occurrence” of points in clusters. This similarity can be used to detect arbitrary shaped \\nclusters.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'PDFCreator Version 0.9.3', 'creationdate': '2009-08-05T10:21:00-04:00', 'moddate': 'D:20090805102100', 'title': 'Jain_PRL_paper_Aug4', 'author': 'Abhishek', 'keywords': '', 'subject': '', 'source': 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\DSC630\\\\why-kmeans-research-paper.pdf', 'total_pages': 35, 'page': 19, 'page_label': '20'}, page_content='To appear in Pattern Recognition Letters, 2009.  \\ninformation is in the form of pair-wise constraints. A must-link constraint  specifies that \\nthe point pair connected by the constraint belong to the same cluster. On the other hand, a \\ncannot-link constraint  specifies that the point pair connected by the constraint do not \\nbelong to the same cluster. It is generally assumed that the constraints are provided by the \\ndomain expert. There is limited work on automatically deriving constraints from the data. \\nSome attempts to derive constraints from domain ontology and other external sources \\ninto clustering algorithms include the usage of WordNet ontology, gene ontology, \\nWikipedia, etc. to guide clustering solutions. However, these are mostly feature \\nconstraints and not constraints on the instances [Hotho et al., 2003, Liu et al., 2004, \\nBanerjee et al., 2007b]. Other approaches for including side information include (i) \\n“seeding”, where some labeled data is used along with large amount of unlabeled data for \\nbetter clustering [Basu et al., 2002] and (ii) methods that allow encouraging or \\ndiscouraging some links [Law et al., 2005, Figueiredo et al., 2006]. \\nFigure 12 illustrates the semi-supervised learning in an image segmentation application \\n[6].  The textured image to be segmented (clustered) is shown in Figure 12 (a). In \\naddition to the image, a set of user-specified pair-wise constraints on the pixel labels are \\nalso provided. Figure 12 (b) shows the clustering obtained when no constraints are used, \\nwhile Figure 12 (c) shows improved clustering with the use of constraints. In both the \\ncases, the number of clusters was assumed to be known (K = 5). \\n \\n \\n (a) Input image and constraints                  (b) No constraints                        (c) 10% pixels in constraints \\n \\nFigure 12 Semi-supervised learning. (a) Input image consisting of five homogeneous textured regions; \\nexamples of must-link (solid blue lines) and must not link (broken red lines) constraints between pixels to \\nbe clustered are specified. (b) 5-cluster solution (segmentation) without constraints. (c) Improved clustering \\n(with five clusters) with 10% of the data points included in the pair-wise constraints [6]. \\nMost approaches [Bar-Hillel et al., 2003, Basu et al., 2004, Chapelle et al., 2006, Lu & \\nLeen, 2007] to semi-supervised clustering modify the objective function of existing \\nclustering algorithms to incorporate the pair-wise constraints. It is desirable to have an \\napproach to semi-supervised clustering that can improve the performance of an already \\nexisting clustering algorithm without modifying it. BoostCluster  [Liu et al., 2007] adopts \\nthis philosophy and follows a boosting framework to improve the performance of any'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'PDFCreator Version 0.9.3', 'creationdate': '2009-08-05T10:21:00-04:00', 'moddate': 'D:20090805102100', 'title': 'Jain_PRL_paper_Aug4', 'author': 'Abhishek', 'keywords': '', 'subject': '', 'source': 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\DSC630\\\\why-kmeans-research-paper.pdf', 'total_pages': 35, 'page': 20, 'page_label': '21'}, page_content='To appear in Pattern Recognition Letters, 2009.  \\ngiven clustering algorithm using pair-wise constraints. It iteratively modifies the input to \\nthe clustering algorithm by generating new data representations (transforming the n x n \\nsimilarity matrix) such that the pair-wise constraints are satisfied while also maintaining \\nthe integrity of the clustering output. Figure 13 shows the performance of BoostCluster \\nevaluated on handwritten digit database in the UCI repository [Blake & Merz, 1998] with \\n4,000 points in 256-dimensional feature space. BoostCluster is able to improve the \\nperformance of all the three commonly used clustering algorithms, K-means, single-link, \\nand Spectral clustering as pair-wise constraints are added to the data. Only must-link \\nconstraints are specified here and the number of true clusters is assumed to be known \\n(K=10). \\n \\n \\n \\nFigure 13 Performance of BoostCluster (measured using Normalized Mutual Information (NMI)) as the \\nnumber of pair-wise constraints are increased. The three plots correspond to boosted performance of  K-\\nmeans, Single-Link (SLINK), and Spectral clustering (SPEC).  \\n \\n4.3 Large scale clustering \\n \\nLarge-scale data clustering addresses the challenge of clustering millions of data points \\nthat are represented in thousands of features. Table 1 shows a few examples of real-world \\napplications for large-scale data clustering. Below, we review the application of large-\\nscale data clustering to content-based image retrieval. \\n \\n \\nApplication Description # Objects # Features \\ndocument clustering group documents of similar topics \\n[Andrews & Fox, 2007] \\n10 6 10 4 \\ngene clustering group genes with similar expression \\nlevels[A. V. Lukashin & Fuchs, 2003] \\n10 5 10 2 \\ncontent-based image \\nretrieval \\nquantize low-level image \\nfeatures[J. Philbin & Zisserman, 2007] \\n10 \\n9 10 2 \\nclustering of earth  \\nscience data \\nderive climate indices[Michael Steinbach \\n& Potter, 2003] \\n10 \\n5 10 2'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'PDFCreator Version 0.9.3', 'creationdate': '2009-08-05T10:21:00-04:00', 'moddate': 'D:20090805102100', 'title': 'Jain_PRL_paper_Aug4', 'author': 'Abhishek', 'keywords': '', 'subject': '', 'source': 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\DSC630\\\\why-kmeans-research-paper.pdf', 'total_pages': 35, 'page': 21, 'page_label': '22'}, page_content='To appear in Pattern Recognition Letters, 2009.  \\n \\nTable 1: Example applications of large-scale data clustering \\n \\n \\n \\n \\n \\n \\nFigure 14 Three  tattoo images represented using SIFT key points.  (a) a pair of similar images has 370 \\nmatching key points; (b) a pair of different images has 64 matching key points. The green lines show the \\nmatching key-points between the images [Lee et al., 2008]. \\n \\nThe goal of Content Based Image Retrieval (CBIR) is to retrieve visually similar images \\nto a given query image. Although the topic has been studied for the past 15 years or so, \\nthere has been only limited success. Most early work on CBIR was based on computing \\ncolor, shape , and texture based features and using them to define a similarity between the \\nimages. A 2008 survey on CBIR highlights the different approaches used for CBIR \\nthrough time [Datta et al., 2008]. Recent approaches for CBIR use key point based \\nfeatures. For example, SIFT [Lowe, 2004] descriptors can be used to represent the images \\n(see Figure 14). However, once the size of the image database increases (~10 million), \\nand assuming 10 milliseconds to compute the matching score between an image pair, a \\nlinear search would take approximately 30 hours to answer one query. This clearly is \\nunacceptable. \\n \\nOn the other hand, text retrieval applications are much faster. It takes about one-tenth of a \\nsecond to search 10 billion documents in Google. A novel approach for image retrieval is \\nto convert the problem into a text retrieval problem. The key points from all the images \\nare first clustered into a large number of clusters (which is usually much less than the \\n \\n(a) 370 \\n \\n(b) 64'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'PDFCreator Version 0.9.3', 'creationdate': '2009-08-05T10:21:00-04:00', 'moddate': 'D:20090805102100', 'title': 'Jain_PRL_paper_Aug4', 'author': 'Abhishek', 'keywords': '', 'subject': '', 'source': 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\DSC630\\\\why-kmeans-research-paper.pdf', 'total_pages': 35, 'page': 22, 'page_label': '23'}, page_content='To appear in Pattern Recognition Letters, 2009.  \\nnumber of key points themselves). These are called visual words.  An image is then \\nrepresented by a histogram of visual words, i.e., the number of key-points from the image \\nthat are in each word or each cluster. By representing each image by a histogram of \\nvisual words, we can then cast the problem of image search into a problem of text \\nretrieval and exploit text search engines for efficient image retrieval. One of the major \\nchallenges in quantizing key points is the number of objects to be clustered. For a \\ncollection of 1,000 images with an average of 1,000 key points and target number of \\n5,000 visual words, it requires clustering 10 6 objects into 5,000 clusters. \\n \\nA large number of clustering algorithms have been developed to efficiently handle large-\\nsize data sets. Most of these studies can be classified into four categories: \\n \\n/square6 Efficient Nearest Neighbor (NN) Search. One of the basic operations in any data \\nclustering algorithm is to decide the cluster membership of each data point, which \\nrequires NN search. Algorithms for efficient NN search are either tree-based (e.g. kd-\\ntree [Moore, 1998, Muja & Lowe, 2009]) or random projection based (e.g., Locality \\nSensitive Hash [Buhler, 2001]). \\n/square6 Data Summarization. Objective here is to improve the clustering efficiency by first \\nsummarizing a large data set into a relatively small subset, and then applying the \\nclustering algorithms to the summarized data set. Example algorithms include \\nBIRCH [Zhang et al., 1996], divide-and-conquer [Steinbach et al., 2000], coreset K-\\nmeans [Har-peled & Mazumdar, 2004], and coarsening methods [Karypis & Kumar, \\n1995]. \\n/square6 Distributed Computing. Approaches in this category [Dhillon & Modha, 1999] \\ndivide each step of a data clustering algorithm into a number of procedures that can \\nbe computed independently. These independent computational procedures will then \\nbe carried out in parallel by different processors to reduce the overall computation \\ntime. \\n/square6 Incremental Clustering. These algorithms, for example, [Bradley et al., 1998] are \\ndesigned to operate in a single pass over data points to improve the efficiency of data \\nclustering. This is in contrast to most clustering algorithms that require multiple \\npasses over data points before identifying the cluster centers. COBWEB is a popular \\nhierarchical clustering algorithm that does a single pass through the available data \\nand arranges it into a classification tree incrementally [Fisher, 1987]. \\n/square6 Sampling-based methods. Algorithms like CURE [Guha et al., 1998, Kollios et al., \\n2003] subsample a large dataset selectively, and perform clustering over the smaller \\nset, which is later transferred to the larger dataset. \\n \\n4.4 Multi-way clustering \\n \\nObjects or entities to be clustered are often formed by a combination of related \\nheterogeneous components. For example, a document is made of words, title, authors, \\ncitations, etc. While objects can be converted into a pooled feature vector of its \\ncomponents prior to clustering, it is not a natural representation of the objects and may \\nresult in poor clustering performance.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'PDFCreator Version 0.9.3', 'creationdate': '2009-08-05T10:21:00-04:00', 'moddate': 'D:20090805102100', 'title': 'Jain_PRL_paper_Aug4', 'author': 'Abhishek', 'keywords': '', 'subject': '', 'source': 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\DSC630\\\\why-kmeans-research-paper.pdf', 'total_pages': 35, 'page': 23, 'page_label': '24'}, page_content='To appear in Pattern Recognition Letters, 2009.  \\nCo-clustering [Hartigan, 1972, Mirkin, 1996] aims to cluster both features and instances \\nof the data (or both rows and columns of the n x d pattern matrix) simultaneously to \\nidentify the subset of features where the resulting clusters are meaningful according to \\ncertain evaluation criterion. This problem was first studied under the name direct \\nclustering  by Hartigan [Hartigan, 1972]. It is also called bi-dimensional clustering  \\n[Cheng & Church, 2000], double clustering , coupled clustering  or bimodal clustering . \\nThis notion is also related to subspace clustering where all the clusters are identified in a \\ncommon subspace. Co-clustering is most popular in the field of bioinformatics, especially \\nin gene clustering, and has also been successfully applied to document clustering [Slonim \\n& Tishby, 2000, Dhillon et al., 2003].  \\n \\nThe co-clustering framework was extended to \\nmulti-way clustering  in [Bekkerman et al., \\n2005] to cluster a set of objects by simultaneously clustering their heterogeneous \\ncomponents. Indeed, the problem is much more challenging because different pairs of \\ncomponents may participate in different types of similarity relationships. In addition, \\nsome relations may involve more than two components. Banerjee et al. [Banerjee et al., \\n2007a] present a family of multi-way clustering schemes that is applicable to a class of \\nloss functions known as Bregman divergences. Sindhwani et al.[Sindhwani et al., 2008] \\napply semi-supervised learning in the co-clustering framework. \\n \\n4.5 Heterogeneous data \\n \\nIn traditional pattern recognition settings, a feature vector consists of measurements of \\ndifferent properties of an object. This representation of objects is not a natural \\nrepresentation for several types of data. Heterogeneous data refers to the data where the \\nobjects may not be naturally represented using a fixed length feature vector. \\n \\nRank Data: Consider a dataset generated by ranking of a set of n movies by different \\npeople; only some of the n objects are ranked. The task is to cluster the users whose \\nrankings are similar and also to identify the ‘representative rankings’ of each group \\n[Mallows, 1957, Critchlow, 1985, Busse et al., 2007]. \\n \\nDynamic Data: \\nDynamic data, as opposed to static data, can change over the course of \\ntime e.g., blogs, Web pages, etc. As the data gets modified, clustering must be updated \\naccordingly. A data stream  is a kind of dynamic data that is transient in nature, and \\ncannot be stored on a disk. Examples include network packets received by a router and \\nstock market, retail chain, or credit card transaction streams. Characteristics of the data \\nstreams include their high volume and potentially unbounded size, sequential access and \\ndynamically evolving nature. This imposes additional requirements to traditional \\nclustering algorithms to rapidly process and summarize the massive amount of \\ncontinuously arriving data. It also requires the ability to adapt to changes in the data \\ndistribution, the ability to detect emerging clusters and distinguish them from outliers in \\nthe data, and the ability to merge old clusters or discard expired ones. All of these \\nrequirements make data stream clustering a significant challenge since they are expected \\nto be single-pass algorithms [Guha et al., 2003b]. Because of the high-speed processing \\nrequirements, many of the data stream clustering methods [Guha et al., 2003a, Aggarwal'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'PDFCreator Version 0.9.3', 'creationdate': '2009-08-05T10:21:00-04:00', 'moddate': 'D:20090805102100', 'title': 'Jain_PRL_paper_Aug4', 'author': 'Abhishek', 'keywords': '', 'subject': '', 'source': 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\DSC630\\\\why-kmeans-research-paper.pdf', 'total_pages': 35, 'page': 24, 'page_label': '25'}, page_content='To appear in Pattern Recognition Letters, 2009.  \\net al., 2003, Cao et al., 2006, Hore et al., 2009b] are extensions of simple algorithms such \\nas K-means, K-medoid, fuzzy c-means or density-based clustering, modified to work in a \\ndata stream environment setting.   \\n \\nGraph Data: Several objects, such as chemical compounds, protein structures, etc. can \\nbe represented most naturally as graphs. Many of the initial efforts in graph clustering \\nhave focused on extracting graph features to allow existing clustering algorithms to be \\napplied to the graph feature vectors [Tsuda & Kudo, 2006]. The features can be extracted \\nbased on patterns such as frequent subgraphs, shortest paths, cycles, and tree-based \\npatterns. With the emergence of kernel learning, there have been growing efforts to \\ndevelop kernel functions that are more suited for graph-based data [Kashima et al., 2003]. \\nOne way to determine the similarity between graphs is by aligning their corresponding \\nadjacency matrix representations [Umeyama, 1988].  \\n \\nRelational Data: Another area that has attracted considerable interest is clustering \\nrelational (network) data. Unlike the clustering of graph data, where the objective is to \\npartition a collection of graphs into disjoint groups, the task here is to partition a large \\ngraph (i.e., network) into cohesive subgraphs based on their link structure and node \\nattributes. The problem becomes even more complicated when the links (which represent \\nrelations between objects) are allowed to have diverse types. One of the key issues is to \\ndefine an appropriate clustering criterion for relational data. A general probabilistic \\nmodel for relational data was first proposed in [Taskar et al., 2001], where different \\nrelated entities are modeled distributions conditioned on each other. Newman’s \\nmodularity function [Newman & Girvan, 2004, Newman, 2006] is a widely-used \\ncriterion for finding community structures in networks, but the measure considers only \\nthe link structure and ignores attribute similarities. A spectral relaxation to Newman and \\nGirvan’s objective function [Newman & Girvan, 2004] for network graph clustering is \\npresented in [White & Smyth, 2005]. Since real networks are often dynamic, another \\nissue is to model the evolutionary behavior of networks, taking into account changes in \\nthe group membership and other characteristic features [L.Backstrom et al., 2006].  \\n \\n5.         Summary \\n \\nOrganizing data into sensible groupings arises naturally in many scientific fields. It is, \\ntherefore, not surprising to see the continued popularity of data clustering. It is important \\nto remember that cluster analysis is an exploratory tool; the output of clustering \\nalgorithms only suggest hypotheses. While numerous clustering algorithms have been \\npublished and new ones continue to appear, there is no single clustering algorithm that \\nhas been shown to dominate other algorithms across all application domains. Most \\nalgorithms, including the simple K-means, are admissible algorithms. As new \\napplications have emerged, it has become increasingly clear that the task of seeking the \\nbest clustering principle might indeed be futile. As an example, consider the application \\ndomain of enterprise knowledge management. Given the same set of document corpus, \\ndifferent user groups (e.g., legal, marketing, management, etc) may be interested in \\ngenerating partitions of documents based on their respective needs. A clustering method \\nthat satisfies the needs for one group may violate the needs of another. As mentioned'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'PDFCreator Version 0.9.3', 'creationdate': '2009-08-05T10:21:00-04:00', 'moddate': 'D:20090805102100', 'title': 'Jain_PRL_paper_Aug4', 'author': 'Abhishek', 'keywords': '', 'subject': '', 'source': 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\DSC630\\\\why-kmeans-research-paper.pdf', 'total_pages': 35, 'page': 25, 'page_label': '26'}, page_content='To appear in Pattern Recognition Letters, 2009.  \\nearlier \"clustering is in the eye of the beholder\" - so indeed data clustering must involve \\nthe user or application needs.  \\n \\nClustering has numerous success stories in data analysis. In spite of this, machine \\nlearning and pattern recognition communities need to address a number of issues to \\nimprove our understanding of data clustering. Below is a list of problems and research \\ndirections that are worth focusing in this regard. \\n \\n(a) There needs to be a suite of benchmark data (with ground truth) available for the \\nresearch community to test and evaluate clustering methods. The benchmark should \\ninclude data sets from various domains (documents, images, time series, customer \\ntransactions, biological sequences, social networks, etc).  Benchmark should also include \\nboth static and dynamic data (the latter would be useful in analyzing clusters that change \\nover time), quantitative and/or qualitative attributes, linked and non-linked objects, etc. \\nThough the idea of providing a benchmark data is not new (e.g., UCI ML and KDD \\nrepository), current benchmarks are limited to small, static data sets. \\n \\n(b) We need to achieve a tighter integration between clustering algorithm and the \\napplication needs. For example, some applications may require generating only a few \\ncohesive clusters (less cohesive clusters can be ignored), while others may require the \\nbest partition of the entire data. In most applications, it may not necessarily be the best \\nclustering algorithm that really matters. Rather, it is more crucial to choose the right \\nfeature extraction method that identifies the underlying clustering structure of the data. \\n \\n(c) Regardless of the principle (or objective), most clustering methods are eventually cast \\ninto combinatorial optimization problems that aim to find the partitioning of data that \\noptimizes the objective. As a result, computational issue becomes critical when the \\napplication inviolves large scale data. For instance, finding the global optimal solution for \\nK-means is NP hard. Hence, it is important to choose clustering principles  that lead to \\ncomputationally efficient solutions.  \\n \\n(d) A fundamental issue related to clustering is its stability or consistency. A good \\nclustering principle should result in a data partitioning that is stable with respect to \\nperturbations in the data. We need to develop clustering methods that lead to stable \\nsolutions.  \\n \\n(e) Choosing clustering principles according to their satisfiability of the stated axioms. \\nDespite Kleinberg’s impossibility theorem, several studies have shown that it can be \\novercome by relaxing  some of the axioms. Thus, maybe one way to evaluate a clustering \\nprinciple is to determine to what degree it satisfies the axioms.  \\n \\n(f) Given the inherent difficulty of clustering, it makes more sense to develop semi-\\nsupervised clustering techniques in which the labeled data and (user specified) pairwise \\nconstraints can be used to decide both (i) data representation and (ii) appropriate \\nobjective function for data clustering.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'PDFCreator Version 0.9.3', 'creationdate': '2009-08-05T10:21:00-04:00', 'moddate': 'D:20090805102100', 'title': 'Jain_PRL_paper_Aug4', 'author': 'Abhishek', 'keywords': '', 'subject': '', 'source': 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\DSC630\\\\why-kmeans-research-paper.pdf', 'total_pages': 35, 'page': 26, 'page_label': '27'}, page_content='To appear in Pattern Recognition Letters, 2009.  \\n \\nAcknowledgement \\n \\nI would like to acknowledge the  National Science Foundation and the Office of Naval \\nresearch for supporting my research in data clustering, dimensionality reduction, \\nclassification, and semi-supervised learning.  I am grateful to Rong Jin, Pang-Ning Tan \\nand Pavan Mallapragada for helping me prepare the King-Sun Fu lecture as well as this \\nmanuscript. I have learned much from and enjoyed my fruitful collaborations in data \\nclustering with Eric Backer, Joachim Buhmann, (late) Richard Dubes, Mario Figueiredo, \\nPatrick Flynn, Ana Fred, Martin Law, J. C. Mao, M. Narasimha Murty, Steve Smith, and \\nAlexander Topchy. Joydeep Ghosh, Larry Hall, Jianying Hu, Mario Figueredo and Ana \\nfred provided many useful suggestions to improve the quality of the paper. \\n \\n \\n \\nReferences \\n \\n[gsc, 2009] . 2009 (February). \\nGoogle Scholar . http://scholar.google.com. \\n[jst, 2009] . 2009. JSTOR . http://www.jstor.org. \\n[A. V. Lukashin & Fuchs, 2003] A. V. LUKASHIN , M.  E. LUKASHEV , &  FUCHS , R. \\n2003. Topology of gene expression networks as revealed by data mining and modeling. \\nBioinformatics , 19 (15), 1909–1916. \\n[Aggarwal et al. , 2003] A GGARWAL , C HARU  C.,  H AN , JIAWEI , W ANG , JIANYONG , &  \\nY U , PHILIP  S. 2003. A framework for clustering evolving data streams. Pages 81–92 of:  \\nProceedings of the 29th International Conference on Very Large Data Bases . \\n[Agrawal et al. , 1998] AGRAWAL , R AKESH , G EHRKE , JOHANNES , G UNOPULOS , D IMITRIOS , \\n&  R AGHAVAN , PRABHAKAR . 1998. Automatic subspace clustering of high dimensional \\ndata for data mining applications. Pages 94–105 of:  Proceedings of the ACM SIGMOD . \\n[Anderberg, 1973] A NDERBERG , M. R. 1973. Cluster analysis for applications . \\nAcademic Press. \\n[Andrews & Fox, 2007] A\\nNDREWS , N ICHOLAS  O.,  &  FOX , EDWARD  A. 2007. Recent \\ndevelopments in document clustering . Tech. rept. TR-07-35. Deparment of Computer \\nScience, Virginia Tech. \\n[Arabie & Hubert, 1994] A\\nRABIE , P.,  &  H UBERT , L. 1994. Advanced methods in \\nmarketing research . Oxford: Blackwell. Chap. Cluster Analysis in Marketing Research, \\npages 160–189. \\n[Backer, 1978]B\\nACKER , ERIC . 1978. Cluster analysis by optimal decomposition of \\ninduced fuzzy sets . Delft University Press. \\n[Baldi & Hatfield, 2002] BALDI , P.,  &  H ATFIELD , G. 2002. DNA microarrays and \\ngene expression . Cambridge University Press. \\n[Ball & Hall, 1965] BALL , G.,  &  H ALL , D. 1965. ISODATA, a novel method of data \\nanlysis and pattern classification . Tech. rept. NTIS AD 699616. Stanford Research \\nInstitute, Stanford, CA. \\n[Banerjee \\net al. , 2004] BANERJEE , A RINDAM , M ERUGU , SRUJANA , D HILLON , INDERJIT , &  \\nG HOSH , JOYDEEP . 2004. Clustering with bregman divergences. Journal of Machine \\nLearning Research , 234–245.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'PDFCreator Version 0.9.3', 'creationdate': '2009-08-05T10:21:00-04:00', 'moddate': 'D:20090805102100', 'title': 'Jain_PRL_paper_Aug4', 'author': 'Abhishek', 'keywords': '', 'subject': '', 'source': 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\DSC630\\\\why-kmeans-research-paper.pdf', 'total_pages': 35, 'page': 27, 'page_label': '28'}, page_content='To appear in Pattern Recognition Letters, 2009.  \\n[Banerjee et al. , 2007a] B ANERJEE , A RINDAM , B ASU , SUGATO , &  M ERUGU , \\nSRUJANA . 2007a. Multi-way clustering on relation graphs. In:  Proceedings of the 7th \\nSIAM International Conference on Data Mining . \\n[Banerjee et al. , 2007b] B ANERJEE , S.,  R AMANATHAN , K.,  &  G UPTA , A. 2007b. \\nClustering short texts using Wikipedia. In:  Proceedings of the SIGIR . \\n[Bar-Hillel et al. , 2003] B AR -HILLEL , A ARON , H ERTZ , T.,  SHENTAL , N OAM , &  \\nW EINSHALL , D APHNA . 2003. Learning distance functions using equivalence relations. \\nPages 11–18 of:  Proceedings of 20th International Conference on Machine Learning \\n(ICML) . \\n[Basu et al. , 2002] B ASU , SUGATO , B ANERJEE , A RINDAM , &  M OONEY , R AYMOND . \\n2002. Semi-supervised clustering by seeding. In:  Proceedings of 19th International \\nConference on Machine Learning (ICML) . \\n[Basu et al. , 2004] B ASU , SUGATO , B ILENKO , M IKHAIL , &  M OONEY , R AYMOND  J. \\n2004. A probabilistic framework for semi-supervised clustering. Pages 59–68 of:  \\nProceedings of the tenth ACM SIGKDD International Conference on Knowledge \\nDiscovery and Data Mining . New York, NY, USA: ACM. \\n[Basu et al. , 2008] B ASU , SUGATO , D AVIDSON , IAN , &  W AGSTAFF , K IRI  (eds). 2008. \\nConstrained clustering: Advances in algorithms, theory and applications . Data Mining \\nand Knowledge Discovery, vol. 3. Chapman & Hall/CRC. \\n[Bekkerman et al. , 2005] B EKKERMAN , R ON , EL-YANIV , R AN , &  M C C ALLUM , \\nA NDREW . 2005. Multi-way distributional clustering via pairwise interactions. Pages 41–\\n48 of:  Proceedings of the 22nd International Conference on Machine learning (ICML) . \\nNew York, NY, USA: ACM. \\n[Belkin & Niyogi, 2002] B\\nELKIN , M IKHAIL , &  N IYOGI , PARTHA . 2002. Laplacian \\neigenmaps and spectral techniques for embedding and clustering. Pages 585–591 of:  \\nAdvances in Neural Information Processing Systems 14 . MIT Press. \\n[Bezdek, 1981] B EZDEK , J. C. 1981. Pattern recognition with fuzzy objective \\nfunction algorithms . Plenum Press. \\n[Bhatia & Deogun, 1998] B HATIA , S.,  &  D EOGUN , J. 1998. Conceputal clustering in \\ninformation retrieval. IEEE Transactions on Systems, Man and Cybernetics , 28 (B), 427–\\n436. \\n[Bishop, 2006] B\\nISHOP , C HRISTOPHER  M. 2006. Pattern recognition and machine \\nlearning . Springer. \\n[Blake & Merz, 1998] BLAKE , &  M ERZ , C. J. 1998. UCI repository of machine learning \\ndatabases . \\n[Blei et al. , 2003] B LEI , D. M.,  N G , A. Y.,  &  JORDAN , M. I. 2003. Latent dirichlet \\nallocation. Journal of machine learning research , 3, 993–1022. \\n[Bradley et al. , 1998] BRADLEY , P. S.,  FAYYAD , U.,  &  R EINA , C. 1998. Scaling clustering \\nalgorithms to large databases. In:  Fourth International Conference on Knowledge \\nDiscovery and Data Mining . \\n[Buhler, 2001] BUHLER , J. 2001. Efficient large-scale sequence comparison by locality-\\nsensitive hashing. Bioinformatics , 17 (5), 419–428. \\n[Busse et al. , 2007] B USSE , LUDWIG  M.,  O RBANZ , PETER , &  B UHMANN , JOACHIM  M. \\n2007. Cluster analysis of heterogeneous rank data. Pages 113–120 of:  Proceedings of the \\n24th International Conference on Machine learning (ICML) . New York, NY, USA: \\nACM.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'PDFCreator Version 0.9.3', 'creationdate': '2009-08-05T10:21:00-04:00', 'moddate': 'D:20090805102100', 'title': 'Jain_PRL_paper_Aug4', 'author': 'Abhishek', 'keywords': '', 'subject': '', 'source': 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\DSC630\\\\why-kmeans-research-paper.pdf', 'total_pages': 35, 'page': 28, 'page_label': '29'}, page_content='To appear in Pattern Recognition Letters, 2009.  \\n[Cao et al. , 2006] C AO , F.,  ESTER , M.,  Q IAN , W.,  &  ZHOU , A. 2006. Density-based \\nclustering over an evolving data stream with noise. In:  Proc. SIAM Conf. Data Mining . \\n[Chapelle et al. , 2006] CHAPELLE , O.,  SCHÖLKOPF , B.,  &  ZIEN , A. (eds). 2006. Semi-\\nsupervised learning . Cambridge, MA: MIT Press. \\n[Cheng & Church, 2000] C HENG , Y IZONG , &  C HURCH , G EORGE  M. 2000. \\nBiclustering of expression data. Pages 93–103 of:  Proceedings of the Eighth \\nInternational Conference on Intelligent Systems for Molecular Biology . AAAI Press. \\n[Connell & Jain, 2002] C ONNELL , S.D.,  &  JAIN , A.K. 2002. Writer adaptation for \\nonline handwriting recognition. IEEE Transactions on Pattern Analysis and Machine \\nIntelligence , 24 (3), 329–346. \\n[Critchlow, 1985] C RITCHLOW , D. 1985. Metric methods for analyzing partially \\nranked data . Springer. \\n[Datta et al. , 2008] D ATTA , R ITENDRA , JOSHI , D HIRAJ , LI, JIA , &  W ANG , JAMES  Z. \\n2008. Image retrieval: Ideas, influences, and trends of the new age. ACM Computing \\nSurveys , 40 (2), article 5. \\n[Dempster et al. , 1977] D EMPSTER , A. P.,  LAIRD , N. M.,  &  R UBIN , D. B. 1977. \\nMaximum likelihood from incomplete data via the EM algorithm. Journal of the Royal \\nStatistical Society , 39 , 1–38. \\n[Dhillon & Modha, 1999] D HILLON , I.,  &  M ODHA , D. 1999. A data-clustering \\nalgorithm on distributed memory multiprocessors. Pages 245–260 of:  Proceedings of the \\nKDD’99 Workshop on High Performance Knowledge Discovery . \\n[Dhillon et al. , 2003] DHILLON , INDERJIT  S.,  M ALLELA , SUBRAMANYAM , G UYON , \\nISABELLE , &  ELISSEEFF , A NDRÉ . 2003. A divisive information-theoretic feature clustering \\nalgorithm for text classification. Journal of Machine Learning Research , 3, 2003. \\n[Dhillon et al. , 2004] DHILLON , INDERJIT  S.,  G UAN , Y UQIANG , &  K ULIS , B RIAN . 2004. \\nKernel k-means: spectral clustering and normalized cuts. Pages 551–556 of:  Proceedings \\nof the tenth ACM SIGKDD International Conference on Knowledge Discovery and Data \\nMining . New York, NY, USA: ACM Press. \\n[Ding et al. , 2005] D ING , C HRIS , H E, X IAOFENG , &  SIMON , H ORST  D. 2005. On the \\nequivalence of nonnegative matrix factorization and spectral clustering. Pages 606–610 \\nof:  Proc. SIAM International Conference on Data Mining . \\n[Drineas et al. , 1999] DRINEAS , P.,  FRIEZE , A.,  K ANNAN , R.,  V EMPALA , S.,  &  V INAY , V. \\n1999. Clustering large graphs via the singular value decomposition. Machine Learning , \\n56 (1-3), 9–33. \\n[Dubes & Jain, 1976] DUBES , &  JAIN . 1976. Clustering techniques: User’s dilemma. \\nPattern Recognition , 247–260. \\n[Duda et al. , 2001] D UDA , R.,  H ART , P.,  &  STORK , D. 2001. Pattern classification . 2 \\nedn. New York: John Wiley & Sons. \\n[Dunn, 1973] D\\nUNN , J. C. 1973. A fuzzy relative of the ISODATA process and its use in \\ndetecting compact well-separated clusters. Journal of Cybernetics , 3, 32–57. \\n[Eschrich et al. , 2003] ESCHRICH , S.,  K E, JINGWEI , H ALL , L.O.,  &  G OLDGOF , D.B. 2003. \\nFast accurate fuzzy clustering through data reduction. IEEE Transactions on Fuzzy \\nSystems , 11 (2), 262–270. \\n[Ester et al. , 1996] E STER , M ARTIN , PETER K RIEGEL , H ANS , S,  JÖRG , &  X U , X IAOWEI . \\n1996. A density-based algorithm for discovering clusters in large spatial databases with'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'PDFCreator Version 0.9.3', 'creationdate': '2009-08-05T10:21:00-04:00', 'moddate': 'D:20090805102100', 'title': 'Jain_PRL_paper_Aug4', 'author': 'Abhishek', 'keywords': '', 'subject': '', 'source': 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\DSC630\\\\why-kmeans-research-paper.pdf', 'total_pages': 35, 'page': 29, 'page_label': '30'}, page_content='To appear in Pattern Recognition Letters, 2009.  \\nnoise. Pages 226–231 of:  Proceedings of the 2nd International Conference on \\nKnowledge Discovery and Data mining . AAAI Press. \\n[Ferguson, 1973] F ERGUSON , THOMAS  S. 1973. A bayesian analysis of some \\nnonparametric problems. Annals of Statistics , 1, 209–230. \\n[Figueiredo et al. , 2006] F IGUEIREDO , M.  A. T.,  C HANG , D. S.,  &  M URINO , V. 2006. \\nClustering under prior knowledge with application to image segmentation. Pages 401–\\n408 of:  NIPS 19 . \\n[Figueiredo & Jain, 2002] FIGUEIREDO , M ARIO , &  JAIN , A NIL  K. 2002. Unsupervised \\nlearning of finite mixture models. IEEE Transactions on Pattern Analysis and Machine \\nIntelligence , 24 (3), 381–396. \\n[Fisher, 1987] FISHER , D OUGLAS  H. 1987. Knowledge acquisition via incremental \\nconceptual clustering. Pages 139–172 of:  Machine Learning . \\n[Fisher & vanNess, 1971] FISHER , L.,  &  VAN N ESS , J. 1971. Admissible clustering \\nprocedures. Biometrika . \\n[Forgy, 1965] FORGY , E. W. 1965. Cluster analysis of multivariate data: efficiency vs \\ninterpretability of classifications. Biometrics , 21 , 768–769. \\n[Frank & Todeschini, 1994] FRANK , ILDIKO  E.,  &  TODESCHINI , R OBERTO . 1994. Data \\nanalysis handbook . Elsevier Science Inc. Pages 227–228. \\n[Fred & Jain, 2002] FRED , A.,  &  JAIN , A.K. 2002. Data clustering using evidence \\naccumulation. In:  Proceedings of the International Conference on Pattern Recognition \\n(ICPR) . \\n[Frigui & Krishnapuram, 1999] FRIGUI , H.,  &  K RISHNAPURAM , R. 1999. A robust \\ncompetitive clustering algorithm with applications in computer vision. IEEE \\nTransactions on Pattern Analysis and Machine Intelligence , 21 , 450–465. \\n[Gantz, 2008] GANTZ , JOHN  F. 2008 (March). The diverse and exploding digital \\nuniverse . Available online at : http://www.emc.com/collateral/analyst-reports/diverse-\\nexploding-digital-universe.pdf. \\n[Guha \\net al. , 2003a] GUHA , S.,  M EYERSON , A.,  M ISHRA , N.,  M OTWANI , R.,  &  \\nL. O’C ALLAGHAN  ”,  TKDE (2003). 2003a. Clustering data streams: Theory and practice. \\nTransactions on Knowledge Discovery and Engieering . \\n[Guha et al. , 1998] G UHA , SUDIPTO , R ASTOGI , R AJEEV , &  SHIM , K YUSEOK . 1998 \\n(June). CURE: an efficient clustering algorithm for large databases. Pages 73–84 of:  \\nACM SIGMOD International Conference on Management of Data . \\n[Guha et al. , 2000] G UHA , SUDIPTO , R ASTOGI , R AJEEV , &  SHIM , K YUSEOK . 2000. \\nRock: A robust clustering algorithm for categorical attributes. Information Systems , \\n25 (5), 345 – 366. \\n[Guha et al. , 2003b] GUHA , SUDIPTO , M ISHRA , N INA , &  M OTWANI , R AJEEV . 2003b. \\nClustering data streams. IEEE Transactions on Knowledge and Data Engineering, 15 (3), \\n515–528. \\n[Hagen & Kahng, 1992] H\\nAGEN , L.,  &  K AHNG , A.B. 1992. New spectral methods \\nfor ratio cut partitioning and clustering. IEEE transactions on computer-aided design of \\nintegrated circuits and systems , 11 (9), 1074–1085. \\n[Han & Kamber, 2000] H AN , JIAWEI , &  K AMBER , M ICHELINE . 2000. Data mining: \\nConcepts and techniques . Morgan Kaufmann.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'PDFCreator Version 0.9.3', 'creationdate': '2009-08-05T10:21:00-04:00', 'moddate': 'D:20090805102100', 'title': 'Jain_PRL_paper_Aug4', 'author': 'Abhishek', 'keywords': '', 'subject': '', 'source': 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\DSC630\\\\why-kmeans-research-paper.pdf', 'total_pages': 35, 'page': 30, 'page_label': '31'}, page_content='To appear in Pattern Recognition Letters, 2009.  \\n[Hansen & Yu, 2001] HANSEN , M ARK  H.,  &  Y U , B IN . 2001. Model selection and the \\nprinciple of minimum description length. Journal of the American Statistical Association , \\n96 (454), 746–774. \\n[Har-peled & Mazumdar, 2004] H AR -PELED , SARIEL , &  M AZUMDAR , SOHAM . 2004. \\nCoresets for k-means and k-median clustering and their applications. Pages 291–300 of:  \\nIn Proc. 36th Annu. ACM Sympos. Theory Comput . \\n[Hartigan, 1972] H ARTIGAN , J. A. 1972. Direct clustering of a data matrix. Journal \\nof the American Statistical Association , 67 (337), 123–132. \\n[Hartigan, 1975] H ARTIGAN , J.A. 1975. Clustering algorithms . John Wiley & Sons. \\n[Hofmann & Buhmann, 1997] H OFMANN , T.,  &  B UHMANN , J.M. 1997. Pairwise \\ndata clustering by deterministic annealing. IEEE Transactions on Pattern Analysis and \\nMachine Intelligence , 19 (1), 1–14. \\n[Hore et al. , 2009a] H ORE , PRODIP , H ALL , LAWRENCE  O.,  &  G OLDGOF , D MITRY  B. \\n2009a. A scalable framework for cluster ensembles. Pattern Recogn. , 42 (5), 676–688. \\n[Hore et al. , 2009b] H ORE , PRODIP , H ALL , LAWRENCE  O.,  G OLDGOF , D MITRY  B.,  G U , \\nY UHUA , M AUDSLEY , A NDREW  A.,  &  D ARKAZANLI , A MMAR . 2009b. A scalable \\nframework for segmenting magnetic resonance images. J. Signal Process. Syst. , 54 (1-3), \\n183–203. \\n[Hotho \\net al. , 2003] H OTHO , A.,  STAAB , S.,  &  STUMME , G. 2003. Ontologies to improve \\ntext document clustering. In:  Proceedings of the ICDM . \\n[Hu et al. , 2007] H U , J.,  R AY , B. K.,  &  SINGH , M. 2007. Statistical methods for \\nautomated generation of service engagement staffing plans. IBM J. Res. Dev. , 51 (3), 281–\\n293. \\n[Iwayama & Tokunaga, 1995] I\\nWAYAMA , M.,  &  TOKUNAGA , T. 1995. Cluster-\\nbased text categorization: a comparison of category search strategies. Pages 273–281 of:  \\nProceedings of the 18th ACM International Conference on Research and Development in \\nInformation Retrieval . \\n[J. Philbin & Zisserman, 2007] J. PHILBIN , O. CHUM , M.  ISARD J. SIVIC , &  \\nZISSERMAN , A. 2007. Object retrieval with large vocabularies and fast spatial matching. \\nIn:  Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . \\n[J. W. Sammon, 1969] J. W. S AMMON , JR . 1969. A nonlinear mapping for data \\nstructure analysis. IEEE Transactions on Computers , 18 , 401–409. \\n[Jain et al. , 2004] J AIN , A. K.,  TOPCHY , A.,  LAW , M.  H. C.,  &  B UHMANN , J. M. 2004. \\nLandscape of clustering algorithms. Pages 260–263 of:  Proceedings of the International \\nConference on Pattern Recognition , vol. 1. \\n[Jain & Dubes, 1988] JAIN , A NIL  K.,  &  D UBES , R ICHARD  C. 1988. Algorithms for \\nclustering data . Prentice Hall. \\n[Jain & Flynn, 1996] JAIN , A NIL  K.,  &  FLYNN , P. 1996. Advances in image \\nunderstanding . IEEE Computer Society Press. Chap. Image segmentation using \\nclustering, pages 65–83. \\n[Karypis & Kumar, 1995] K\\nARYPIS , G EORGE , &  K UMAR , V IPIN . 1995. A fast and high \\nquality multilevel scheme for partitioning irregular graphs. Pages 113–122 of:  \\nProceedings of the International Conference on Parallel Processing . \\n[Kashima et al. , 2003] KASHIMA , H.,  TSUDA , K.,  &  INOKUCHI , A. 2003. Marginalized \\nkernels between labeled graphs. Pages 321–328 of:  Proceedings of the Twentieth \\nInternational Conference on Machine Learning .'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'PDFCreator Version 0.9.3', 'creationdate': '2009-08-05T10:21:00-04:00', 'moddate': 'D:20090805102100', 'title': 'Jain_PRL_paper_Aug4', 'author': 'Abhishek', 'keywords': '', 'subject': '', 'source': 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\DSC630\\\\why-kmeans-research-paper.pdf', 'total_pages': 35, 'page': 31, 'page_label': '32'}, page_content='To appear in Pattern Recognition Letters, 2009.  \\n[Kashima et al. , 2008] KASHIMA , H.,  H U , J.,  R AY , B.,  &  SINGH , M. 2008 (Dec.). K-means \\nclustering of proportional data using L1 distance. \\n[Kaufman & Rousseeuw, 2005] K\\nAUFMAN , LEONARD , &  R OUSSEEUW , PETER  J. \\n2005. Finding groups in data : An introduction to cluster analysis . Wiley series in \\nProbability and Statistics. \\n[Kleinberg, 2002] K\\nLEINBERG , JON . 2002. An impossibility theorem for clustering. \\nPages 463–470 of:  NIPS 15 . MIT Press. \\n[Kollios et al. , 2003] KOLLIOS , G.,  G UNOPULOS , D.,  K OUDAS , N.,  &  B ERCHTOLD , S. \\n2003. Efficient biased sampling for approximate clustering and outlier detection in large \\ndata sets. IEEE Transactions on Knowledge and Data Engineering, 15 (5), 1170–1187. \\n[Lange et al., 2005] LANGE , T.,  LAW , M. H.,  JAIN , A. K.,  &  B UHMANN , J. 2005 (June). \\nLearning with constrained and unlabelled data. Pages 730–737 of:  IEEE Computer \\nSociety Conference on Computer Vision and Pattern Recognition , vol. 1. \\n[Lange et al. , 2004] LANGE , TILMAN , R OTH , V OLKER , B RAUN , M IKIO  L.,  &  B UHMANN , \\nJOACHIM  M. 2004. Stability-based validation of clustering solutions. Neural \\nComputation , 16 (6), 1299–1323. \\n[Law et al. , 2005] L AW , M ARTIN , TOPCHY , A LEXANDER , &  JAIN , A. K. 2005. Model-\\nbased clustering with probabilistic constraints. Pages 641–645 of:  Proc. SIAM \\nConference on Data Mining . \\n[L.Backstrom et al. , 2006] L.B ACKSTROM , H UTTENLOCHER , D.,  K LEINBERG , J.,  &  LAN , \\nX. 2006. Group formation in large social networks: Membership, growth, and evolution. \\nIn:  Proceedings of the KDD . \\n[Lee et al. , 2008] L EE , JUNG -EUN , JAIN , A NIL  K.,  &  JIN , R ONG . 2008. Scars, marks \\nand tattoos (SMT): Soft biometric for suspect and victim identification. In:  Proceedings \\nof the Biometric Symposium . \\n[Li & McCallum, 2006] L I, W.,  &  M C C ALLUM , A. 2006. Pachinko allocation: Dag-\\nstructured mixture models of topic correlations. Pages 577––584 of:  Proceedings of the \\n23rd International Conference on Machine Learning . \\n[Linde et al. , 1980] L INDE , Y.,  B UZO , A.,  &  G RAY , R. 1980. An algorithm for vector \\nquantizer design. IEEE Transactions on Communications , 28 , 84–94. \\n[Liu et al. , 2004] L IU , J.,  W ANG , W.,  &  Y ANG , J. 2004. A framework for ontology-\\ndriven subspace clustering. In:  Proceedings of the KDD . \\n[Liu et al. , 2007] L IU , Y I, JIN , R ONG , &  JAIN , A. K. 2007. Boostcluster: Boosting \\nclustering by pairwise constraints. Pages 450–459 of:  Proceedings of the 13th ACM \\nSIGKDD International Conference on Knowledge Discovery and Data Mining (KDD) . \\n[Lloyd, 1982] LLOYD , S. 1982. Least squares quantization in PCM. IEEE Transactions \\non Information Theory , 28 , 129–137. Originally as an unpublished Bell laboratories \\nTechnical Note (1957). \\n[Lowe, 2004] L\\nOWE , D AVID  G. 2004. Distinctive image features from scale-invariant \\nkeypoints. International Journal of Computer Vision , 60 (2), 91–110. \\n[Lu & Leen, 2007] L U , ZHENGDONG , &  LEEN , TODD  K. 2007. Penalized probabilistic \\nclustering. Neural Comput. , 19 (6), 1528–1567. \\n[M. Welling & Hinton, 2005] M. WELLING , M.  R OSEN -ZVI , &  H INTON , G. 2005. \\nExponential family harmoniums with an application to information retrieval. Page 1481–\\n1488 of:  Advances in Neural Information Processing Systems 17 .'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'PDFCreator Version 0.9.3', 'creationdate': '2009-08-05T10:21:00-04:00', 'moddate': 'D:20090805102100', 'title': 'Jain_PRL_paper_Aug4', 'author': 'Abhishek', 'keywords': '', 'subject': '', 'source': 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\DSC630\\\\why-kmeans-research-paper.pdf', 'total_pages': 35, 'page': 32, 'page_label': '33'}, page_content='To appear in Pattern Recognition Letters, 2009.  \\n[MacQueen, 1967] M AC Q UEEN , J. 1967. Some methods for classification and analysis \\nof multivariate observations. Pages 281–297 of:  Fifth Berkeley Symposium on \\nMathematics, Statistics and Probability . University of California Press. \\n[Mallows, 1957] M ALLOWS , C. L. 1957. Non-null ranking models. Biometricka , 44 , \\n114–130. \\n[Mao & Jain, 1996] M\\nAO , J.,  &  JAIN , A.K. 1996. A self-organizing network for hyper-\\nellipsoidal clustering (hec). IEEE Transactions on Neural Networks , 7(Jan), 16–29. \\n[McLachlan & Basford, 1987] M C LACHLAN , G. L.,  &  B ASFORD , K. E. 1987. \\nMixture models: Inference and applications to clustering . Marcel Dekker. \\n[Meila, 2003] MEILA , M ARINA . 2003. Comparing clusterings by the variation of \\ninformation. Pages 173–187 of:  COLT . \\n[Meila, 2006] MEILA , M ARINA . 2006. The uniqueness of a good optimum for k-means. \\nPages 625–632 of:  Proceedings of the 23rd International Conference on Machine \\nLearning . \\n[Meila & Shi, 2001] MEILA , M ARINA , &  SHI , JIANBO . 2001. A random walks view of \\nspectral segmentation. In:  Proceedings of the AISTATAS . \\n[Merriam-Webster Online Dictionary, 2008] MERRIAM -W EBSTER O NLINE D ICTIONARY . \\n2008 (Feb). \"cluster analysis\" . http://www.merriam-webster-online.com. \\n[Michael Steinbach & Potter, 2003] MICHAEL  STEINBACH , PANG -NING  TAN , V IPIN \\nK UMAR STEVE  KLOOSTER , &  POTTER , C HRISTOPHER . 2003. Discovery of climate indices \\nusing clustering. In:  Proceedings of the ninth acm sigkdd international conf on \\nknowledge discovery and data mining . \\n[Mirkin, 1996] MIRKIN , B ORIS . 1996. Mathematical classification and clustering . Kluwer \\nAcademic Publishers. \\n[Moore, 1998] M\\nOORE , A NDREW  W. 1998. Very fast EM-based mixture model clustering \\nusing multiresolution kd-trees. Pages 543–549 of:  NIPS . \\n[Muja & Lowe, 2009] MUJA , M.,  &  LOWE , D. G. 2009. Fast approximate nearest \\nneighbors with automatic algorithm configuration. In:  Proccedings of the International \\nConference on Computer Vision Theory and Applications (VISAPP’09) . \\n[Newman & Girvan, 2004] N EWMAN , M.,  &  G IRVAN , M. 2004. Finding and evaluating \\ncommunity structure in networks. Physical Review E, , 69 (026113). \\n[Newman, 2006] N EWMAN , M.  E. J. 2006. Modularity and community structure in \\nnetworks. In:  Proc. National Academy of Sciences, USA . \\n[Ng et al. , 2001] N G , A NDREW  Y.,  JORDAN , M ICHAEL  I.,  &  W EISS , Y AIR . 2001. On \\nspectral clustering: Analysis and an algorithm. Pages 849–856 of:  Advances in Neural \\nInformation Processing Systems 14 . MIT Press. \\n[Pampalk et al. , 2003] PAMPALK , ELIAS , D IXON , SIMON , &  W IDMER , G ERHARD . 2003. On \\nthe evaluation of perceptual similarity measures for music. Pages 7–12 of:  Proceedings \\nof the Sixth International Conference on Digital Audio Effects (DAFx-03) . London, UK: \\nQueen Mary University of London. \\n[Pelleg & Moore, 1999] P\\nELLEG , D AN , &  M OORE , A NDREW . 1999. Accelerating \\nexact k-means algorithms with geometric reasoning. Pages 277–281 of:  CHAUDHURI , \\nSURAJIT , &  M ADIGAN , D AVID  (eds), Proceedings of the Fifth International Conference on \\nKnowledge Discovery in Databases . AAAI Press. \\n[Pelleg & Moore, 2000] P ELLEG , D AN , &  M OORE , A NDREW . 2000. X-means: \\nExtending k-means with efficient estimation of the number of clusters. Pages 727–734'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'PDFCreator Version 0.9.3', 'creationdate': '2009-08-05T10:21:00-04:00', 'moddate': 'D:20090805102100', 'title': 'Jain_PRL_paper_Aug4', 'author': 'Abhishek', 'keywords': '', 'subject': '', 'source': 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\DSC630\\\\why-kmeans-research-paper.pdf', 'total_pages': 35, 'page': 33, 'page_label': '34'}, page_content='To appear in Pattern Recognition Letters, 2009.  \\nof:  Proceedings of the Seventeenth International Conference on Machine Learning . San \\nFrancisco: Morgan Kaufmann. \\n[Rasmussen, 2000] R\\nASMUSSEN , C ARL . 2000. The infinite gaussian mixture model. \\nAdvances in Neural Information Processing Systems (NIPS) , 12 , 554–560. \\n[Roberts et al. , 2001] ROBERTS , STEPHEN  J.,  H OLMES , C HRISTOPHER , &  D ENISON , D AVE . \\n2001. Minimum-entropy data clustering using reversible jump markov chain monte carlo. \\nPages 103–110 of:  ICANN ’01: Proceedings of the International Conference on Artificial \\nNeural Networks . London, UK: Springer-Verlag. \\n[S. Ben-David, 2008] S. BEN -DAVID , M. ACKERMAN . 2008. Measures of clustering \\nquality: A working set of axioms for clustering. In:  Neural Information Processing \\nSystems . \\n[Sahami, 1998] S AHAMI , M EHRAN . 1998. Using machine learning to improve \\ninformation access . Ph.D. thesis, Computer Science Department, Stanford University. \\n[Scholkopf et al. , 1998] S CHOLKOPF , B ERNHARD , SMOLA , A LEXANDER , &  M ULLER , \\nK LAUS -ROBERT . 1998. Nonlinear component analysis as a kernel eigenvalue problem. \\nNeural Computation , 10 (5), 1299–1319. \\n[Shamir & Tishby, 2008] SHAMIR , O HAD , &  TISHBY , N AFTALI . 2008. Cluster stability \\nfor finite samples. Pages 1297–1304 of:  PLATT , J.C.,  K OLLER , D.,  SINGER , Y.,  &  R OWEIS , \\nS. (eds), Advances in Neural Information Processing Systems 20 . Cambridge, MA: MIT \\nPress. \\n[Shi & Malik, 2000] S\\nHI, JIANBO , &  M ALIK , JITENDRA . 2000. Normalized cuts and \\nimage segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence , \\n22 , 888–905. \\n[Sindhwani et al. , 2008] S INDHWANI , V.,  H U , J.,  &  M OJSILOVIC , A. 2008. \\nRegularized co-clustering with dual supervision. In:  NIPS . \\n[Slonim & Tishby, 2000] SLONIM , N OAM , &  TISHBY , N AFTALI . 2000. Document \\nclustering using word clusters via the information bottleneck method. Pages 208–215 of:  \\nIn ACM SIGIR 2000 . ACM press. \\n[Smith & Jain, 1984] SMITH , STEPHEN  P.,  &  JAIN , A NIL  K. 1984. Testing for uniformity \\nin multidimensional data. IEEE Transactions on Pattern Analysis and Machine \\nIntelligence , 6(1), 73–81. \\n[Sokal & Sneath, 1963] S OKAL , R OBERT  R.,  &  SNEATH , PETER H. A. 1963. \\nPrinciples of numerical taxonomy . W. H. Freeman, San Francisco. \\n[Steinbach et al. , 2000] S TEINBACH , M.,  K ARYPIS , G.,  &  K UMAR , V. 2000. A \\ncomparison of document clustering techniques. In:  Workshop on KDD . \\n[Steinhaus, 1956] STEINHAUS , H. 1956. Sur la division des corp materiels en parties. \\nBulletin of Acad. Polon. Sci. , IV (C1. III), 801–804. \\n[Strehl & Ghosh, 2003] S TREHL , A LEXANDER , &  G HOSH , JOYDEEP . 2003. Cluster \\nensembles — a knowledge reuse framework for combining multiple partitions. Journal of \\nMachine Learning Research , 3, 583–617. \\n[Tabachnick & Fidell, 2007] TABACHNICK , B. G.,  &  FIDELL , L. S. 2007. Using \\nmultivariate statistics . 5 edn. Boston: Allyn and Bacon. \\n[Tan et al. , 2005] T AN , PANG -NING , STEINBACH , M ICHAEL , &  K UMAR , V IPIN . 2005. \\nIntroduction to data mining . First edn. Boston, MA, USA: Addison-Wesley Longman \\nPublishing Co., Inc.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'PDFCreator Version 0.9.3', 'creationdate': '2009-08-05T10:21:00-04:00', 'moddate': 'D:20090805102100', 'title': 'Jain_PRL_paper_Aug4', 'author': 'Abhishek', 'keywords': '', 'subject': '', 'source': 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\DSC630\\\\why-kmeans-research-paper.pdf', 'total_pages': 35, 'page': 34, 'page_label': '35'}, page_content='To appear in Pattern Recognition Letters, 2009.  \\n[Taskar et al. , 2001] TASKAR , B.,  SEGAL , E.,  &  K OLLER , D. 2001. Probabilistic \\nclustering in relational data. Pages 870–887 of:  Proceedings of the Seventeenth \\nInternational Joint Conference on Arti cial Intelligence (IJCAI) . \\n[Tibshirani et al. , 2001] T IBSHIRANI , R.,  W ALTHER , G.,  , &  H ASTIE , T. 2001. \\nEstimating the number of clusters in a data set via the gap statistic. Journal of the Royal \\nStatistical Society , B , 411–423. \\n[Tishby et al. , 1999] TISHBY , N AFTALI , PEREIRA , FERNANDO  C.,  &  B IALEK , W ILLIAM . \\n1999. The information bottleneck method. Pages 368–377 of:  Proceedings of the 37th \\nAllerton Conference on Communication, Control and Computing . \\n[Tsuda & Kudo, 2006] T SUDA , K OJI , &  K UDO , TAKU . 2006. Clustering graphs by \\nweighted substructure mining. Pages 953–960 of:  Proceedings of the 23rd International \\nConference on Machine learning . New York, NY, USA: ACM. \\n[Tukey, 1977] TUKEY , JOHN  W ILDER . 1977. Exploratory data analysis . Addison-Wesley. \\n[Umeyama, 1988] U MEYAMA , S. 1988. An eigendecomposition approach to weighted \\ngraph matching problems. IEEE Transactions on Pattern Analysis and Machine \\nIntelligence , 10 (5), 695–703. \\n[von Luxburg & David, 2005] VON LUXBURG , U.,  &  D AVID , B EN  S. 2005. Towards \\na statistical theory of clustering. In:  Pascal workshop on statistics and optimization of \\nclustering . \\n[Wallace & Boulton, 1968] WALLACE , C.S,  &  B OULTON , D. M. 1968. An information \\nmeasure for classification. Computing Journal , 11 , 185–195. \\n[Wallace & Freeman, 1987] WALLACE , C.S.,  &  FREEMAN , P.R. 1987. Estimation and \\ninference by compact coding (with discussions). JRSSB , 49 , 240–251. \\n[Wasserman & Faust, 1994] WASSERMAN , S.,  &  FAUST , K. 1994. Social network \\nanalysis: methods and applications . Cambridge University Press. \\n[White & Smyth, 2005] W HITE , SCOTT , &  SMYTH , PADHRAIC . 2005. A spectral \\nclustering approach to finding communities in graph. In:  Proceedings of the SIAM Data \\nMining . \\n[Yu & Shi, 2003] Y U , STELLA  X.,  &  SHI , JIANBO . 2003. Multiclass spectral \\nclustering. Pages 313–319 of:  Proceedings of the International Conference on Computer \\nVision . \\n[Zhang et al. , 1996] ZHANG , TIAN , R AMAKRISHNAN , R AGHU , &  LIVNY , M IRON . 1996. \\nBIRCH: an efficient data clustering method for very large databases. Pages 103–114 of:  \\nProceedings of the 1996 ACM SIGMOD International Conference on Management of \\ndata , vol. 25. New York, NY, USA: ACM.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T00:12:20.039875Z",
     "start_time": "2025-10-05T00:12:20.032135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "doc_obj = pdf_docs[0]\n",
    "doc_obj"
   ],
   "id": "e80016d6015bdc61",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'PDFCreator Version 0.9.3', 'creationdate': '2009-08-05T10:21:00-04:00', 'moddate': 'D:20090805102100', 'title': 'Jain_PRL_paper_Aug4', 'author': 'Abhishek', 'keywords': '', 'subject': '', 'source': 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\DSC630\\\\why-kmeans-research-paper.pdf', 'total_pages': 35, 'page': 0, 'page_label': '1'}, page_content='To appear in Pattern Recognition Letters, 2009.  \\nData Clustering: 50 Years Beyond K-Means1 \\n \\nAnil K. Jain \\nDepartment of Computer Science & Engineering \\nMichigan State University \\nEast Lansing, Michigan 48824 USA \\njain@cse.msu.edu  \\n \\n \\nAbstract:  Organizing data into sensible groupings is one of the most fundamental \\nmodes of understanding and learning. As an example, a common scheme of scientific \\nclassification puts organisms into a system of ranked taxa: domain, kingdom, phylum, \\nclass, etc.. Cluster analysis is the formal study of methods and algorithms for grouping, or \\nclustering, objects according to measured or perceived intrinsic characteristics or \\nsimilarity. Cluster analysis does not use category labels that tag objects with prior \\nidentifiers, i.e., class labels. The absence of category information distinguishes data \\nclustering (unsupervised learning) from classification or discriminant analysis \\n(supervised learning). The aim of clustering is to find structure in data and is therefore \\nexploratory in nature. Clustering has a long and rich history in a variety of scientific \\nfields. One of the most popular and simple clustering algorithms, K-means, was first \\npublished in 1955. In spite of the fact that K-means was proposed over 50 years ago and \\nthousands of clustering algorithms have been published since then, K-means is still  \\nwidely used. This speaks to the difficulty of designing a general purpose clustering \\nalgorithm and the ill-posed problem of clustering. We provide a brief overview of \\nclustering, summarize well known clustering methods, discuss the major challenges and \\nkey issues in designing clustering algorithms, and point out some of the emerging and \\nuseful research directions, including semi-supervised clustering, ensemble clustering, \\nsimultaneous feature selection during data clustering and large scale data clustering. \\n \\n \\n1.  Introduction \\n \\nAdvances in sensing and storage technology and dramatic growth in applications such as \\nInternet search, digital imaging, and video surveillance have created many high-volume, \\nhigh-dimensional data sets. It is estimated that the digital universe consumed \\napproximately 281 exabytes in 2007, and it is projected to be 10 times that size by 2011. \\n(One exabyte is ~10 \\n18 bytes or 1,000,000 terabytes) [Gantz, 2008]. Most of the data is \\nstored digitally in electronic media, thus providing huge potential for the development of \\nautomatic data analysis, classification, and retrieval techniques. In addition to the growth \\nin the amount of data, the variety of available data (text, image, and video) has also \\nincreased. Inexpensive digital and video cameras have made available huge archives of \\n                                                 \\n1 This paper is based on the King-Sun Fu Prize lecture delivered at the 19 th  International \\nConference on Pattern Recognition (ICPR), Tampa, FL, December 8, 2008.')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T00:12:31.417247Z",
     "start_time": "2025-10-05T00:12:31.408201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "Markdown(doc_obj.page_content)"
   ],
   "id": "6744b9d73755df8e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "To appear in Pattern Recognition Letters, 2009.  \nData Clustering: 50 Years Beyond K-Means1 \n \nAnil K. Jain \nDepartment of Computer Science & Engineering \nMichigan State University \nEast Lansing, Michigan 48824 USA \njain@cse.msu.edu  \n \n \nAbstract:  Organizing data into sensible groupings is one of the most fundamental \nmodes of understanding and learning. As an example, a common scheme of scientific \nclassification puts organisms into a system of ranked taxa: domain, kingdom, phylum, \nclass, etc.. Cluster analysis is the formal study of methods and algorithms for grouping, or \nclustering, objects according to measured or perceived intrinsic characteristics or \nsimilarity. Cluster analysis does not use category labels that tag objects with prior \nidentifiers, i.e., class labels. The absence of category information distinguishes data \nclustering (unsupervised learning) from classification or discriminant analysis \n(supervised learning). The aim of clustering is to find structure in data and is therefore \nexploratory in nature. Clustering has a long and rich history in a variety of scientific \nfields. One of the most popular and simple clustering algorithms, K-means, was first \npublished in 1955. In spite of the fact that K-means was proposed over 50 years ago and \nthousands of clustering algorithms have been published since then, K-means is still  \nwidely used. This speaks to the difficulty of designing a general purpose clustering \nalgorithm and the ill-posed problem of clustering. We provide a brief overview of \nclustering, summarize well known clustering methods, discuss the major challenges and \nkey issues in designing clustering algorithms, and point out some of the emerging and \nuseful research directions, including semi-supervised clustering, ensemble clustering, \nsimultaneous feature selection during data clustering and large scale data clustering. \n \n \n1.  Introduction \n \nAdvances in sensing and storage technology and dramatic growth in applications such as \nInternet search, digital imaging, and video surveillance have created many high-volume, \nhigh-dimensional data sets. It is estimated that the digital universe consumed \napproximately 281 exabytes in 2007, and it is projected to be 10 times that size by 2011. \n(One exabyte is ~10 \n18 bytes or 1,000,000 terabytes) [Gantz, 2008]. Most of the data is \nstored digitally in electronic media, thus providing huge potential for the development of \nautomatic data analysis, classification, and retrieval techniques. In addition to the growth \nin the amount of data, the variety of available data (text, image, and video) has also \nincreased. Inexpensive digital and video cameras have made available huge archives of \n                                                 \n1 This paper is based on the King-Sun Fu Prize lecture delivered at the 19 th  International \nConference on Pattern Recognition (ICPR), Tampa, FL, December 8, 2008."
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T00:14:05.243080Z",
     "start_time": "2025-10-05T00:13:55.204993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "embeddings\n",
    "\n",
    "vectordb = Chroma.from_documents(pdf_docs,embedding=embeddings)\n",
    "\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL,temperature=0 )\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n"
   ],
   "id": "26206867e89c330d",
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRateLimitError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m OpenAIEmbeddings()\n\u001B[0;32m      2\u001B[0m embeddings\n\u001B[1;32m----> 4\u001B[0m vectordb \u001B[38;5;241m=\u001B[39m \u001B[43mChroma\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_documents\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpdf_docs\u001B[49m\u001B[43m,\u001B[49m\u001B[43membedding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43membeddings\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m retriever \u001B[38;5;241m=\u001B[39m vectordb\u001B[38;5;241m.\u001B[39mas_retriever()\n\u001B[0;32m      8\u001B[0m llm \u001B[38;5;241m=\u001B[39m ChatOpenAI(model\u001B[38;5;241m=\u001B[39mMODEL,temperature\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m )\n",
      "File \u001B[1;32m~\\gyan-python-workspace\\jup-workspace\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:887\u001B[0m, in \u001B[0;36mChroma.from_documents\u001B[1;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001B[0m\n\u001B[0;32m    885\u001B[0m texts \u001B[38;5;241m=\u001B[39m [doc\u001B[38;5;241m.\u001B[39mpage_content \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m documents]\n\u001B[0;32m    886\u001B[0m metadatas \u001B[38;5;241m=\u001B[39m [doc\u001B[38;5;241m.\u001B[39mmetadata \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m documents]\n\u001B[1;32m--> 887\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_texts\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    888\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtexts\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtexts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    889\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    890\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmetadatas\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetadatas\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    891\u001B[0m \u001B[43m    \u001B[49m\u001B[43mids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    892\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcollection_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollection_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    893\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpersist_directory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpersist_directory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    894\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclient_settings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclient_settings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    895\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    896\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcollection_metadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollection_metadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    897\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    898\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\gyan-python-workspace\\jup-workspace\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:843\u001B[0m, in \u001B[0;36mChroma.from_texts\u001B[1;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001B[0m\n\u001B[0;32m    835\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mchromadb\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbatch_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m create_batches\n\u001B[0;32m    837\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m create_batches(\n\u001B[0;32m    838\u001B[0m         api\u001B[38;5;241m=\u001B[39mchroma_collection\u001B[38;5;241m.\u001B[39m_client,\n\u001B[0;32m    839\u001B[0m         ids\u001B[38;5;241m=\u001B[39mids,\n\u001B[0;32m    840\u001B[0m         metadatas\u001B[38;5;241m=\u001B[39mmetadatas,\n\u001B[0;32m    841\u001B[0m         documents\u001B[38;5;241m=\u001B[39mtexts,\n\u001B[0;32m    842\u001B[0m     ):\n\u001B[1;32m--> 843\u001B[0m         \u001B[43mchroma_collection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_texts\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    844\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtexts\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    845\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmetadatas\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    846\u001B[0m \u001B[43m            \u001B[49m\u001B[43mids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    847\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    848\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    849\u001B[0m     chroma_collection\u001B[38;5;241m.\u001B[39madd_texts(texts\u001B[38;5;241m=\u001B[39mtexts, metadatas\u001B[38;5;241m=\u001B[39mmetadatas, ids\u001B[38;5;241m=\u001B[39mids)\n",
      "File \u001B[1;32m~\\gyan-python-workspace\\jup-workspace\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:277\u001B[0m, in \u001B[0;36mChroma.add_texts\u001B[1;34m(self, texts, metadatas, ids, **kwargs)\u001B[0m\n\u001B[0;32m    275\u001B[0m texts \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(texts)\n\u001B[0;32m    276\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_embedding_function \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 277\u001B[0m     embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_embedding_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed_documents\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    278\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m metadatas:\n\u001B[0;32m    279\u001B[0m     \u001B[38;5;66;03m# fill metadatas with empty dicts if somebody\u001B[39;00m\n\u001B[0;32m    280\u001B[0m     \u001B[38;5;66;03m# did not specify metadata for all texts\u001B[39;00m\n\u001B[0;32m    281\u001B[0m     length_diff \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(texts) \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mlen\u001B[39m(metadatas)\n",
      "File \u001B[1;32m~\\gyan-python-workspace\\jup-workspace\\venv\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:592\u001B[0m, in \u001B[0;36mOpenAIEmbeddings.embed_documents\u001B[1;34m(self, texts, chunk_size, **kwargs)\u001B[0m\n\u001B[0;32m    589\u001B[0m \u001B[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001B[39;00m\n\u001B[0;32m    590\u001B[0m \u001B[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001B[39;00m\n\u001B[0;32m    591\u001B[0m engine \u001B[38;5;241m=\u001B[39m cast(\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdeployment)\n\u001B[1;32m--> 592\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_len_safe_embeddings\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    593\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mengine\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunk_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    594\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\gyan-python-workspace\\jup-workspace\\venv\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:482\u001B[0m, in \u001B[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001B[1;34m(self, texts, engine, chunk_size, **kwargs)\u001B[0m\n\u001B[0;32m    480\u001B[0m batched_embeddings: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mfloat\u001B[39m]] \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m    481\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m _iter:\n\u001B[1;32m--> 482\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    483\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokens\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m_chunk_size\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mclient_kwargs\u001B[49m\n\u001B[0;32m    484\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    485\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, \u001B[38;5;28mdict\u001B[39m):\n\u001B[0;32m    486\u001B[0m         response \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mmodel_dump()\n",
      "File \u001B[1;32m~\\gyan-python-workspace\\jup-workspace\\venv\\Lib\\site-packages\\openai\\resources\\embeddings.py:132\u001B[0m, in \u001B[0;36mEmbeddings.create\u001B[1;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[0;32m    126\u001B[0m             embedding\u001B[38;5;241m.\u001B[39membedding \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mfrombuffer(  \u001B[38;5;66;03m# type: ignore[no-untyped-call]\u001B[39;00m\n\u001B[0;32m    127\u001B[0m                 base64\u001B[38;5;241m.\u001B[39mb64decode(data), dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfloat32\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    128\u001B[0m             )\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[0;32m    130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\n\u001B[1;32m--> 132\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    133\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/embeddings\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    134\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membedding_create_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEmbeddingCreateParams\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    135\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    136\u001B[0m \u001B[43m        \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    137\u001B[0m \u001B[43m        \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    138\u001B[0m \u001B[43m        \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    139\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    140\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpost_parser\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    141\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    142\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mCreateEmbeddingResponse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    143\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\gyan-python-workspace\\jup-workspace\\venv\\Lib\\site-packages\\openai\\_base_client.py:1259\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1245\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpost\u001B[39m(\n\u001B[0;32m   1246\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1247\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1254\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1255\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[0;32m   1256\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[0;32m   1257\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[0;32m   1258\u001B[0m     )\n\u001B[1;32m-> 1259\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\gyan-python-workspace\\jup-workspace\\venv\\Lib\\site-packages\\openai\\_base_client.py:1047\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[1;34m(self, cast_to, options, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1044\u001B[0m             err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mread()\n\u001B[0;32m   1046\u001B[0m         log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRe-raising status error\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 1047\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_status_error_from_response(err\u001B[38;5;241m.\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1049\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m   1051\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcould not resolve response (should never happen)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[1;31mRateLimitError\u001B[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#prompt\n",
    "\n",
    "question_ans_chain = "
   ],
   "id": "7401318962e10f47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "58a6b46a44b88b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "848c41836885ef0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4db013497bf51896",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "65f285d5bdccf914",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "21fccbf05cd682d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "22ea257846e1f5d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ff955536108dbddd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e3f68ada8dc04c01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8eb5e5569a3703c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4b76776892ed8c95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3ce75c53631e6c6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "25b57994824f61d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2b291ec640d13995",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "87c32a7f3db68e71",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "134f59d186fcdb4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "98b7bd610a4bc43a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8c935cd794cab33c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4d8658ece82a746c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9cd8250ba07f6557",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fe1199e502c6bd3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e124a11c2e84c02d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4d0f280b55e87964",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9345ff0bb4e762c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3e18c1f084c04c3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "533b09c88065adb8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3fcd3c318df7b15f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1952a79611a4b733",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "df0a1ae8b8ece935",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1afc6bc81b39fab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "83f86cada6c78c9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "65f40e096fabfadb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d52f3ed200274e3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "63292151f163d42b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a4d93fbaf3124cde",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "116851ac0396f09f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e440f73f2be9a7d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "af15f35c4191fa3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c3c7fec9e5eae34e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "132e635052e32f03",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fbedea44586ccb7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ae8ba48539b2a91f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1505055547fce441",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
