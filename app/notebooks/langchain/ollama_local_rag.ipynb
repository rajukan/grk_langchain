{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# # Langchain package\n",
    "# %pip install -qU langchain\n",
    "# \n",
    "# # Local vector store via Chroma\n",
    "# %pip install -qU langchain_chroma\n",
    "# \n",
    "# # Local inference and embeddings via Ollama\n",
    "# %pip install -qU langchain_ollama\n",
    "# \n",
    "# # Web Loader\n",
    "# %pip install -qU beautifulsoup4"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MODEL=\"llama3.2\"",
   "id": "32cbb8193001cca3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain import hub\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ],
   "id": "9bd4c0aa856c3e9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "anamoly_links=[\n",
    "    \"https://medium.com/simform-engineering/anomaly-detection-with-unsupervised-machine-learning-3bcf4c431aff\",\n",
    "    \"https://www.stratascratch.com/blog/machine-learning-algorithms-explained-anomaly-detection/\"\n",
    "]\n",
    "\n",
    "anamology_loader =WebBaseLoader(anamoly_links)"
   ],
   "id": "e80016d6015bdc61",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "anamoly_langchain_docs = anamology_loader.load_and_split()\n",
    "anamoly_langchain_docs"
   ],
   "id": "6744b9d73755df8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.documents import Document\n",
    "from collections import defaultdict\n",
    "\n",
    "#Group docs by source\n",
    "grouped_docs = defaultdict(list)\n",
    "for doc in anamoly_langchain_docs:\n",
    "    source = doc.metadata.get('source', '')\n",
    "    grouped_docs[source].append(doc)\n"
   ],
   "id": "26206867e89c330d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for k,v in grouped_docs.items():\n",
    "    print(f\"[{k=}] {v=}\")"
   ],
   "id": "7401318962e10f47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Combine documents with the same source\n",
    "combined_docs = []\n",
    "for source, docs in grouped_docs.items():\n",
    "    combined_content = \"\\n\".join(doc.page_content for doc in docs)\n",
    "    combined_metadata={}\n",
    "    # combined_metadata = docs[0].metadata.copy()  # Use metadata from the first document\n",
    "    combined_metadata['num_chunks'] = len(docs)  # Add number of original chunks\n",
    "    combined_docs.append(Document(page_content=combined_content, metadata=combined_metadata))\n",
    "\n",
    "# Replace langchain_docs with the combined documents\n",
    "langchain_docs = combined_docs"
   ],
   "id": "58a6b46a44b88b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(langchain_docs)",
   "id": "848c41836885ef0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "anamoly_doc_obj=langchain_docs[0]\n",
    "anamoly_doc_obj"
   ],
   "id": "4db013497bf51896",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "anamoly_doc_obj.page_content",
   "id": "65f285d5bdccf914",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from IPython.display import  Markdown\n",
    "Markdown(anamoly_doc_obj.page_content)"
   ],
   "id": "21fccbf05cd682d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=MODEL)"
   ],
   "id": "22ea257846e1f5d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vectordb = Chroma.from_documents(langchain_docs, embedding=embeddings)\n",
    "vectordb"
   ],
   "id": "ff955536108dbddd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "retriever = vectordb.as_retriever()\n",
    "retriever   "
   ],
   "id": "e3f68ada8dc04c01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "local_llm=ChatOllama(model=MODEL,temperature=0)\n",
    "type(local_llm)"
   ],
   "id": "8eb5e5569a3703c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt"
   ],
   "id": "4b76776892ed8c95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "question_ans_chain = create_stuff_documents_chain(local_llm,prompt)\n",
    "\n",
    "question_ans_chain"
   ],
   "id": "3ce75c53631e6c6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# query =\"Do you know anything about anomaly?\"\n",
    "query =\"How can you use anomaly deduction for monitoring an application latency?\"\n",
    "\n",
    "rag_chain = create_retrieval_chain(retriever,question_ans_chain)\n"
   ],
   "id": "25b57994824f61d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results = rag_chain.invoke({\"input\" : query})\n",
    "results"
   ],
   "id": "7bedc2ea600395cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "final_answer = results[\"answer\"]\n",
    "Markdown(final_answer)"
   ],
   "id": "2b291ec640d13995",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', system_prompt),\n",
    "    ('human', '{input}')\n",
    "])\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "        {\n",
    "            'input': lambda x: x['input'],\n",
    "            'context': lambda x: format_docs(x['context']),\n",
    "        }\n",
    "        | prompt\n",
    "        | local_llm\n",
    "        | StrOutputParser()\n",
    ")"
   ],
   "id": "87c32a7f3db68e71",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# passing the input query to the retriever\n",
    "retrieve_docs = (lambda x: x['input']) | retriever"
   ],
   "id": "134f59d186fcdb4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "chain = RunnablePassthrough.assign(context=retrieve_docs).assign(\n",
    "    answer=rag_chain_from_docs\n",
    ")\n",
    "chain"
   ],
   "id": "98b7bd610a4bc43a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "query = \"Tell me if anomaly deduction falls under machine learning?\"\n",
    "chain.invoke({'input': query})"
   ],
   "id": "8c935cd794cab33c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4d8658ece82a746c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9cd8250ba07f6557",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fe1199e502c6bd3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e124a11c2e84c02d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4d0f280b55e87964",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9345ff0bb4e762c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3e18c1f084c04c3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "533b09c88065adb8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3fcd3c318df7b15f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1952a79611a4b733",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "df0a1ae8b8ece935",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1afc6bc81b39fab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "83f86cada6c78c9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "65f40e096fabfadb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d52f3ed200274e3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "63292151f163d42b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a4d93fbaf3124cde",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "116851ac0396f09f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e440f73f2be9a7d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "af15f35c4191fa3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c3c7fec9e5eae34e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "132e635052e32f03",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fbedea44586ccb7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ae8ba48539b2a91f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1505055547fce441",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
