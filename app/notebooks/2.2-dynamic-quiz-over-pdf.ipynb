{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T12:42:32.249354Z",
     "start_time": "2025-10-05T12:42:21.514593Z"
    }
   },
   "source": [
    "%pip install langchain\n",
    "%pip install langchain-openai\n",
    "%pip install langchainhub\n",
    "%pip install pypdf\n",
    "%pip install chromadb"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from langchain) (0.3.77)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from langchain) (0.3.11)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from langchain) (0.4.31)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from langchain) (2.11.9)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from langchain) (2.0.36)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from langchain) (2.32.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.12.2)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (24.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.27.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (2.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-openai in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (0.3.34)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=0.3.77 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from langchain-openai) (0.3.77)\n",
      "Requirement already satisfied: openai<3.0.0,>=1.104.2 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from langchain-openai) (2.1.0)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from langchain-openai) (0.11.0)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=0.3.77->langchain-openai) (0.4.31)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=0.3.77->langchain-openai) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=0.3.77->langchain-openai) (1.33)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=0.3.77->langchain-openai) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=0.3.77->langchain-openai) (4.12.2)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=0.3.77->langchain-openai) (24.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=0.3.77->langchain-openai) (2.11.9)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (0.11.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (4.67.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.32.5)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.104.2->langchain-openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.104.2->langchain-openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.104.2->langchain-openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<3.0.0,>=1.104.2->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=0.3.77->langchain-openai) (2.4)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.77->langchain-openai) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.77->langchain-openai) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.77->langchain-openai) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=0.3.77->langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=0.3.77->langchain-openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=0.3.77->langchain-openai) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from tqdm>4->openai<3.0.0,>=1.104.2->langchain-openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchainhub in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (0.1.21)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from langchainhub) (24.0)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from langchainhub) (2.32.5)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from langchainhub) (2.32.4.20250913)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from requests<3,>=2->langchainhub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from requests<3,>=2->langchainhub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from requests<3,>=2->langchainhub) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from requests<3,>=2->langchainhub) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (6.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromadb in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from chromadb) (1.3.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from chromadb) (2.11.9)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from chromadb) (1.4.2)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.37.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from chromadb) (1.26.4)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from chromadb) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from chromadb) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from chromadb) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from chromadb) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from chromadb) (1.37.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from chromadb) (0.22.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from chromadb) (4.67.0)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from chromadb) (1.70.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from chromadb) (5.0.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from chromadb) (0.19.2)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from chromadb) (34.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from chromadb) (9.0.0)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from chromadb) (6.0.1)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from chromadb) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from chromadb) (3.11.3)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from chromadb) (0.27.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from chromadb) (4.22.0)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from build>=1.0.3->chromadb) (24.0)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: anyio in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from httpx>=0.27.0->chromadb) (4.4.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.18.1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.41.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.32.5)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: urllib3<2.4.0,>=1.24.2 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.1.24)\n",
      "Requirement already satisfied: protobuf in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.5.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.37.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.58b0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.4.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.35.3)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.12.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.20.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.3.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T14:30:16.326252Z",
     "start_time": "2025-10-05T14:30:16.142252Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "def _set_env_from_file(var: str, file_path: str = \"openai_key.txt\"):\n",
    "    \"\"\"\n",
    "    Reads an API key from a specified file and sets it as an environment variable.\n",
    "    \"\"\"\n",
    "    if not os.environ.get(var):\n",
    "        try:\n",
    "            # The 'with open' statement ensures the file is closed automatically\n",
    "            with open(file_path, 'r') as f:\n",
    "                # Read the first line and strip any leading/trailing whitespace\n",
    "                key = f.readline().strip()\n",
    "\n",
    "            if key:\n",
    "                os.environ[var] = key\n",
    "                print(f\"Successfully loaded {var} from {file_path}\")\n",
    "            else:\n",
    "                print(f\"Warning: {file_path} is empty.\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Key file not found at {file_path}. Please create the file.\")\n",
    "\n",
    "# --- Execution ---\n",
    "# Set the environment variable OPENAI_API_KEY from the file\n",
    "_set_env_from_file('OPENAI_API_KEY')"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up a study workflow using Jupyter Notebooks, LLMs, and langchain."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T14:30:34.858771Z",
     "start_time": "2025-10-05T14:30:34.810383Z"
    }
   },
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T14:31:43.268110Z",
     "start_time": "2025-10-05T14:31:43.262099Z"
    }
   },
   "source": "MODEL=\"gpt-4o-mini\"",
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T14:30:43.001984Z",
     "start_time": "2025-10-05T14:30:42.600232Z"
    }
   },
   "source": [
    "pdf_path = \"C:\\\\Users\\\\gyanr\\\\Downloads\\\\pdf_langchain_test.pdf\"\n",
    "loader = PyPDFLoader(pdf_path) # LOAD\n",
    "pdf_docs = loader.load_and_split() # SPLIT\n",
    "pdf_docs"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-05T12:35:19+00:00', 'source': 'C:\\\\Users\\\\gyanr\\\\Downloads\\\\pdf_langchain_test.pdf', 'total_pages': 4, 'page': 0, 'page_label': '1'}, page_content='that suggested above. This short paper overviews some of the recent work on boosting, focusing\\nespecially on the AdaBoost algorithm which has undergone intense theoretical study and empirical\\ntesting. After introducing AdaBoost, we describe some of the basic underlying theory of boosting,\\nincluding an explanation of why it often tends not to overﬁt. We also describe some experiments\\nand applications using boosting.\\nBackground\\nBoosting has its roots in a theoretical framework for studying machine learning called the “PAC”\\nlearning model, due to Valiant [46]; see Kearns and Vazirani [32] for a good introduction to this\\nmodel. Kearns and Valiant [30, 31] were the ﬁrst to pose the question of whether a “weak” learn-\\ning algorithm which performs just slightly better than random guessing in the PAC model can be\\n“boosted” into an arbitrarily accurate “strong” learning algorithm. Schapire [38] came up with the\\nﬁrst provable polynomial-time boosting algorithm in 1989. A year later, Freund [17] developed\\na much more efﬁcient boosting algorithm which, although optimal in a certain sense, neverthe-\\nless suffered from certain practical drawbacks. The ﬁrst experiments with these early boosting\\nalgorithms were carried out by Drucker, Schapire and Simard [16] on an OCR task.\\nAdaBoost\\nThe AdaBoost algorithm, introduced in 1995 by Freund and Schapire [23], solved many of the\\npractical difﬁculties of the earlier boosting algorithms, and is the focus of this paper. Pseudocode\\nfor AdaBoost is given in Fig. 1. The algorithm takes as input a training set \\x00\\x02\\x01\\x04\\x03\\x06\\x05\\x08\\x07\\t\\x03\\x0b\\n\\x0c\\x05\\x0e\\r\\x0e\\r\\x0e\\r\\x0f\\x05\\x10\\x00\\x11\\x01\\x13\\x12\\x14\\x05\\x08\\x07\\x15\\x12\\x16\\nwhere each \\x01\\x18\\x17 belongs to some domain or instance space \\x19 , and each label \\x07\\t\\x17 is in some label\\nset \\x1a . For most of this paper, we assume \\x1a \\x1b \\x1c\\x1e\\x1d \\x1f!\\x05\\x0c\"#\\x1f%$ ; later, we discuss extensions to the\\nmulticlass case. AdaBoost calls a given weak or base learning algorithm repeatedly in a series of\\nrounds &\\'\\x1b(\\x1f!\\x05\\x0e\\r\\x0e\\r\\x0e\\r\\x0f\\x05*) . One of the main ideas of the algorithm is to maintain a distribution or set of\\nweights over the training set. The weight of this distribution on training example + on round & is\\ndenoted ,.-/\\x00\\x11+0\\n . Initially, all weights are set equally, but on each round, the weights of incorrectly\\nclassiﬁed examples are increased so that the weak learner is forced to focus on the hard examples\\nin the training set.\\nThe weak learner’s job is to ﬁnd a weak hypothesis 12-435\\x19 6 \\x1c\\x1e\\x1d7\\x1f%\\x05\\x0c\"#\\x1f%$ appropriate for the\\ndistribution ,8- . The goodness of a weak hypothesis is measured by its error\\n9\\n-:\\x1b<;>=\\x0b\\x17@?\\x13A2B\\x18CD1E-\\x0b\\x00\\x02\\x01\\x13\\x17\\x11\\nGF \\x1bH\\x07\\x15\\x17JIK\\x1b L\\n\\x17JM N\\x0cBPORQ\\x0cSUTWV\\nXZY\\nS\\n,#-\\x0b\\x00\\x11+0\\n\\x06\\r\\nNotice that the error is measured with respect to the distribution ,[- on which the weak learner\\nwas trained. In practice, the weak learner may be an algorithm that can use the weights ,\\\\- on the\\ntraining examples. Alternatively, when this is not possible, a subset of the training examples can\\nbe sampled according to ,8- , and these (unweighted) resampled examples can be used to train the\\nweak learner.\\nRelating back to the horse-racing example, the instances \\x01:\\x17 correspond to descriptions of horse\\nraces (such as which horses are running, what are the odds, the track records of each horse, etc.)\\n2'),\n",
       " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-05T12:35:19+00:00', 'source': 'C:\\\\Users\\\\gyanr\\\\Downloads\\\\pdf_langchain_test.pdf', 'total_pages': 4, 'page': 1, 'page_label': '2'}, page_content='special case of a more general algorithm for playing repeated games and for approximately solving\\na game. This also shows that boosting is closely related to linear programming and online learning.\\nRelation to support-vector machines\\nThe margin theory points to a strong connection between boosting and the support-vector machines\\nof Vapnik and others [6, 12, 47]. To clarify the connection, suppose that we have already found\\nthe weak hypotheses that we want to combine and are only interested in choosing the coefﬁcients\\n\\x06 - . One reasonable approach suggested by the analysis of AdaBoost’s generalization error is to\\nchoose the coefﬁcients so that the bound given in Eq. (3) is minimized. In particular, suppose\\nthat the ﬁrst term is zero and let us concentrate on the second term so that we are effectively\\nattempting to maximize the minimum margin of any training example.1 To make this idea precise,\\nlet us denote the vector of weak-hypothesis predictions associated with the example \\x00\\x11\\x01 \\x05*\\x07E\\n by\\n\\x00\\n\\x00\\x11\\x01K\\n\\r\\n\\x1b\\x02\\x01 1K\\x03\\x0f\\x00\\x02\\x01K\\n\\x06\\x05\\x0c1\\n\\x07\\n\\x00\\x11\\x01K\\n\\x0c\\x05\\x0e\\r\\x0e\\r\\x0e\\r\\x0f\\x05\\x0c1\\x04\\x03 \\x00\\x02\\x01K\\n\\x06\\x05 which we call the instance vector and the vector of coefﬁcients\\nby \\x07\\n\\r\\n\\x1b\\x08\\x01 \\x06\\x04\\x03\\x06\\x05 \\x06\\n\\x07\\n\\x05\\x0e\\r\\x0e\\r \\r \\x05 \\x06\\t\\x03\\n\\x05 which we call the weight vector. Using this notation and the deﬁnition\\nof margin given in Eq. (2) we can write the goal of maximizing the minimum margin as\\n\\x01\\x03\\x02\\n\\x1d\\n\\x07\\n\\x01\\n%\\n\\n\\x17\\n\\x00\\x0b\\x07\\r\\x0c\\n\\x00\\n\\x00\\x11\\x01\\x13\\x17P\\n\\x0b\\n/\\x07\\x15\\x17\\n8\\x0b8\\x0e\\x07 8\\x0b8 8\\x0b8\\n\\x00\\n\\x00\\x02\\x01\\x13\\x17\\x11\\n 8\\x0b8\\n(4)\\nwhere, for boosting, the norms in the denominator are deﬁned as:\\n8\\x0b8\\x0f\\x07 8 8R\\x03\\n\\r\\n\\x1b<L\\n-\\n8 \\x06 - 8 \\x05 8\\x0b8\\n\\x00\\n\\x00\\x02\\x01K\\n98 8\\x0e\\x10\\n\\r\\n\\x1b\\n\\x01\\x03\\x02\\n\\x1d\\n-\\n8 1Z-\\x0b\\x00\\x11\\x01K\\n98%\\r\\n(When the 1E- ’s all have range \\x1c\\x1e\\x1d \\x1f!\\x05\\x0c\"#\\x1f%$ , 8\\x0b8\\n\\x00\\n\\x00\\x11\\x01K\\n 8\\x0b8\\x0f\\x10 is simply equal to \\x1f .)\\nIn comparison, the explicit goal of support-vector machines is to maximize a minimal margin\\nof the form described in Eq. (4), but where the norms are instead Euclidean:\\n8 8\\x0f\\x07 8 8\\n\\x07\\n\\r\\n\\x1b\\n\\t\\nL\\n-\\n\\x06\\n\\x07\\n-\\n\\x05 8 8\\n\\x00\\n\\x00\\x11\\x01K\\n98 8\\n\\x07\\n\\r\\n\\x1b\\n\\t\\nL\\n-\\n1Z-\\x0b\\x00\\x02\\x01K\\n\\x07\\n\\r\\nThus, SVM’s use the \\x11\\n\\x07\\nnorm for both the instance vector and the weight vector, while AdaBoost\\nuses the \\x11\\x12\\x10 norm for the instance vector and \\x11%\\x03 norm for the weight vector.\\nWhen described in this manner, SVM and AdaBoost seem very similar. However, there are\\nseveral important differences:\\n\\x05\\nDifferent norms can result in very different margins. The difference between the norms\\n\\x11\\x10\\x03 , \\x11\\n\\x07\\nand \\x11\\x12\\x10 may not be very signiﬁcant when one considers low dimensional spaces. How-\\never, in boosting or in SVM, the dimension is usually very high, often in the millions or\\nmore. In such a case, the difference between the norms can result in very large differences\\n1Of course, AdaBoost does not explicitly attempt to maximize the minimal margin. Nevertheless, Schapire\\net al.\\'s [41] analysis suggests that the algorithm does try to make the margins of all the training examples as large\\nas possible, so in this sense, we can regard this maximum minimal margin algorithm as an illustrative approximation\\nof AdaBoost. In fact, algorithms that explicitly attempt to maximize minimal margin have not been experimentally as\\nsuccessful as AdaBoost [7, 27].\\n7'),\n",
       " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-05T12:35:19+00:00', 'source': 'C:\\\\Users\\\\gyanr\\\\Downloads\\\\pdf_langchain_test.pdf', 'total_pages': 4, 'page': 2, 'page_label': '3'}, page_content='in the margin values. This seems to be especially so when there are only a few relevant\\nvariables so that \\x07 can be very sparse. For instance, suppose the weak hypotheses all have\\nrange \\x1c\\x1e\\x1d \\x1f!\\x05\\x0c\"#\\x1f%$ and that the label \\x07 on all examples can be computed by a majority vote of\\n\\x00 \\nof the weak hypotheses. In this case, it can be shown that if the number of relevant weak\\nhypotheses\\n\\x00 \\nis a small fraction of the total number of weak hypotheses then the margin\\nassociated with AdaBoost will be much larger than the one associated with support vector\\nmachines.\\n\\x05\\nThe computation requirements are different. The computation involved in maximizing\\nthe margin is mathematical programming, i.e., maximizing a mathematical expression given\\na set of inequalities. The difference between the two methods in this regard is that SVM cor-\\nresponds to quadratic programming, while AdaBoost corresponds only to linear program-\\nming. (In fact, as noted above, there is a deep relationship between AdaBoost and linear\\nprogramming which also connects AdaBoost with game theory and online learning [22].)\\n\\x05\\nA different approach is used to search efﬁciently in high dimensional space. Quadratic\\nprogramming is more computationally demanding than linear programming. However, there\\nis a much more important computational difference between SVM and boosting algorithms.\\nPart of the reason for the effectiveness of SVM and AdaBoost is that they ﬁnd linear classi-\\nﬁers for extremely high dimensional spaces, sometimes spaces of inﬁnite dimension. While\\nthe problem of overﬁtting is addressed by maximizing the margin, the computational prob-\\nlem associated with operating in high dimensional spaces remains. Support vector machines\\ndeal with this problem through the method of kernels which allow algorithms to perform\\nlow dimensional calculations that are mathematically equivalent to inner products in a high\\ndimensional “virtual” space. The boosting approach is instead to employ greedy search:\\nfrom this perspective, the weak learner is an oracle for ﬁnding coordinates of\\n\\x00\\n\\x00\\x11\\x01K\\n that have\\na non-negligible correlation with the label \\x07 . The reweighting of the examples changes the\\ndistribution with respect to which the correlation is measured, thus guiding the weak learner\\nto ﬁnd different correlated coordinates. Most of the actual work involved in applying SVM\\nor AdaBoost to speciﬁc classiﬁcation problems has to do with selecting the appropriate ker-\\nnel function in the one case and weak learning algorithm in the other. As kernels and weak\\nlearning algorithms are very different, the resulting learning algorithms usually operate in\\nvery different spaces and the classiﬁers that they generate are extremely different.\\nMulticlass classiﬁcation\\nSo far, we have only considered binary classiﬁcation problems in which the goal is to distinguish\\nbetween only two possible classes. Many (perhaps most) real-world learning problems, however,\\nare multiclass with more than two possible classes. There are several methods of extending Ada-\\nBoost to the multiclass case.\\nThe most straightforward generalization [23], called AdaBoost.M1, is adequate when the weak\\nlearner is strong enough to achieve reasonably high accuracy, even on the hard distributions created\\nby AdaBoost. However, this method fails if the weak learner cannot achieve at least 50% accuracy\\nwhen run on these hard distributions.\\n8'),\n",
       " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-05T12:35:19+00:00', 'source': 'C:\\\\Users\\\\gyanr\\\\Downloads\\\\pdf_langchain_test.pdf', 'total_pages': 4, 'page': 3, 'page_label': '4'}, page_content='Experiments and applications\\nPractically, AdaBoost has many advantages. It is fast, simple and easy to program. It has no\\nparameters to tune (except for the number of round ) ). It requires no prior knowledge about\\nthe weak learner and so can be ﬂexibly combined with any method for ﬁnding weak hypotheses.\\nFinally, it comes with a set of theoretical guarantees given sufﬁcient data and a weak learner that\\ncan reliably provide only moderately accurate weak hypotheses. This is a shift in mind set for the\\nlearning-system designer: instead of trying to design a learning algorithm that is accurate over the\\nentire space, we can instead focus on ﬁnding weak learning algorithms that only need to be better\\nthan random.\\nOn the other hand, some caveats are certainly in order. The actual performance of boosting on\\na particular problem is clearly dependent on the data and the weak learner. Consistent with theory,\\nboosting can fail to perform well given insufﬁcient data, overly complex weak hypotheses or weak\\nhypotheses which are too weak. Boosting seems to be especially susceptible to noise [13] (more\\non this later).\\nAdaBoost has been tested empirically by many researchers, including [3, 13, 15, 29, 33, 36, 45].\\nFor instance, Freund and Schapire [21] tested AdaBoost on a set of UCI benchmark datasets [35]\\nusing C4.5 [37] as a weak learning algorithm, as well as an algorithm which ﬁnds the best “decision\\nstump” or single-test decision tree. Some of the results of these experiments are shown in Fig. 3.\\nAs can be seen from this ﬁgure, even boosting the weak decision stumps can usually give as\\ngood results as C4.5, while boosting C4.5 generally gives the decision-tree algorithm a signiﬁcant\\nimprovement in performance.\\nIn another set of experiments, Schapire and Singer [43] used boosting for text categorization\\ntasks. For this work, weak hypotheses were used which test on the presence or absence of a word\\nor phrase. Some results of these experiments comparing AdaBoost to four other methods are\\nshown in Fig. 4. In nearly all of these experiments and for all of the performance measures tested,\\nboosting performed as well or signiﬁcantly better than the other methods tested. Boosting has also\\nbeen applied to text ﬁltering [44], “ranking” problems [19] and classiﬁcation problems arising in\\nnatural language processing [1, 28].\\nThe generalization of AdaBoost by Schapire and Singer [42] provides an interpretation of\\nboosting as a gradient-descent method. A potential function is used in their algorithm to asso-\\nciate a cost with each example based on its current margin. Using this potential function, the\\noperation of AdaBoost can be interpreted as a coordinate-wise gradient descent in the space of\\nlinear classiﬁers (over weak hypotheses). Based on this insight, one can design algorithms for\\nlearning popular classiﬁcation rules. In recent work, Cohen and Singer [11] showed how to apply\\nboosting to learn rule lists similar to those generated by systems like RIPPER [10], IREP [26] and\\nC4.5rules [37]. In other work, Freund and Mason [20] showed how to apply boosting to learn a\\ngeneralization of decision trees called “alternating trees.”\\nA nice property of AdaBoost is its ability to identify outliers, i.e., examples that are either\\nmislabeled in the training data, or which are inherently ambiguous and hard to categorize. Because\\nAdaBoost focuses its weight on the hardest examples, the examples with the highest weight often\\nturn out to be outliers. An example of this phenomenon can be seen in Fig. 5 taken from an OCR\\nexperiment conducted by Freund and Schapire [21].\\nWhen the number of outliers is very large, the emphasis placed on the hard examples can\\n10')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T14:30:43.961433Z",
     "start_time": "2025-10-05T14:30:43.953124Z"
    }
   },
   "source": [
    "doc_obj = pdf_docs[0]\n",
    "doc_obj"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-05T12:35:19+00:00', 'source': 'C:\\\\Users\\\\gyanr\\\\Downloads\\\\pdf_langchain_test.pdf', 'total_pages': 4, 'page': 0, 'page_label': '1'}, page_content='that suggested above. This short paper overviews some of the recent work on boosting, focusing\\nespecially on the AdaBoost algorithm which has undergone intense theoretical study and empirical\\ntesting. After introducing AdaBoost, we describe some of the basic underlying theory of boosting,\\nincluding an explanation of why it often tends not to overﬁt. We also describe some experiments\\nand applications using boosting.\\nBackground\\nBoosting has its roots in a theoretical framework for studying machine learning called the “PAC”\\nlearning model, due to Valiant [46]; see Kearns and Vazirani [32] for a good introduction to this\\nmodel. Kearns and Valiant [30, 31] were the ﬁrst to pose the question of whether a “weak” learn-\\ning algorithm which performs just slightly better than random guessing in the PAC model can be\\n“boosted” into an arbitrarily accurate “strong” learning algorithm. Schapire [38] came up with the\\nﬁrst provable polynomial-time boosting algorithm in 1989. A year later, Freund [17] developed\\na much more efﬁcient boosting algorithm which, although optimal in a certain sense, neverthe-\\nless suffered from certain practical drawbacks. The ﬁrst experiments with these early boosting\\nalgorithms were carried out by Drucker, Schapire and Simard [16] on an OCR task.\\nAdaBoost\\nThe AdaBoost algorithm, introduced in 1995 by Freund and Schapire [23], solved many of the\\npractical difﬁculties of the earlier boosting algorithms, and is the focus of this paper. Pseudocode\\nfor AdaBoost is given in Fig. 1. The algorithm takes as input a training set \\x00\\x02\\x01\\x04\\x03\\x06\\x05\\x08\\x07\\t\\x03\\x0b\\n\\x0c\\x05\\x0e\\r\\x0e\\r\\x0e\\r\\x0f\\x05\\x10\\x00\\x11\\x01\\x13\\x12\\x14\\x05\\x08\\x07\\x15\\x12\\x16\\nwhere each \\x01\\x18\\x17 belongs to some domain or instance space \\x19 , and each label \\x07\\t\\x17 is in some label\\nset \\x1a . For most of this paper, we assume \\x1a \\x1b \\x1c\\x1e\\x1d \\x1f!\\x05\\x0c\"#\\x1f%$ ; later, we discuss extensions to the\\nmulticlass case. AdaBoost calls a given weak or base learning algorithm repeatedly in a series of\\nrounds &\\'\\x1b(\\x1f!\\x05\\x0e\\r\\x0e\\r\\x0e\\r\\x0f\\x05*) . One of the main ideas of the algorithm is to maintain a distribution or set of\\nweights over the training set. The weight of this distribution on training example + on round & is\\ndenoted ,.-/\\x00\\x11+0\\n . Initially, all weights are set equally, but on each round, the weights of incorrectly\\nclassiﬁed examples are increased so that the weak learner is forced to focus on the hard examples\\nin the training set.\\nThe weak learner’s job is to ﬁnd a weak hypothesis 12-435\\x19 6 \\x1c\\x1e\\x1d7\\x1f%\\x05\\x0c\"#\\x1f%$ appropriate for the\\ndistribution ,8- . The goodness of a weak hypothesis is measured by its error\\n9\\n-:\\x1b<;>=\\x0b\\x17@?\\x13A2B\\x18CD1E-\\x0b\\x00\\x02\\x01\\x13\\x17\\x11\\nGF \\x1bH\\x07\\x15\\x17JIK\\x1b L\\n\\x17JM N\\x0cBPORQ\\x0cSUTWV\\nXZY\\nS\\n,#-\\x0b\\x00\\x11+0\\n\\x06\\r\\nNotice that the error is measured with respect to the distribution ,[- on which the weak learner\\nwas trained. In practice, the weak learner may be an algorithm that can use the weights ,\\\\- on the\\ntraining examples. Alternatively, when this is not possible, a subset of the training examples can\\nbe sampled according to ,8- , and these (unweighted) resampled examples can be used to train the\\nweak learner.\\nRelating back to the horse-racing example, the instances \\x01:\\x17 correspond to descriptions of horse\\nraces (such as which horses are running, what are the odds, the track records of each horse, etc.)\\n2')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T14:30:44.907096Z",
     "start_time": "2025-10-05T14:30:44.895667Z"
    }
   },
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "Markdown(doc_obj.page_content)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "that suggested above. This short paper overviews some of the recent work on boosting, focusing\nespecially on the AdaBoost algorithm which has undergone intense theoretical study and empirical\ntesting. After introducing AdaBoost, we describe some of the basic underlying theory of boosting,\nincluding an explanation of why it often tends not to overﬁt. We also describe some experiments\nand applications using boosting.\nBackground\nBoosting has its roots in a theoretical framework for studying machine learning called the “PAC”\nlearning model, due to Valiant [46]; see Kearns and Vazirani [32] for a good introduction to this\nmodel. Kearns and Valiant [30, 31] were the ﬁrst to pose the question of whether a “weak” learn-\ning algorithm which performs just slightly better than random guessing in the PAC model can be\n“boosted” into an arbitrarily accurate “strong” learning algorithm. Schapire [38] came up with the\nﬁrst provable polynomial-time boosting algorithm in 1989. A year later, Freund [17] developed\na much more efﬁcient boosting algorithm which, although optimal in a certain sense, neverthe-\nless suffered from certain practical drawbacks. The ﬁrst experiments with these early boosting\nalgorithms were carried out by Drucker, Schapire and Simard [16] on an OCR task.\nAdaBoost\nThe AdaBoost algorithm, introduced in 1995 by Freund and Schapire [23], solved many of the\npractical difﬁculties of the earlier boosting algorithms, and is the focus of this paper. Pseudocode\nfor AdaBoost is given in Fig. 1. The algorithm takes as input a training set \u0000\u0002\u0001\u0004\u0003\u0006\u0005\b\u0007\t\u0003\u000B\n\f\u0005\u000E\r\u000E\r\u000E\r\u000F\u0005\u0010\u0000\u0011\u0001\u0013\u0012\u0014\u0005\b\u0007\u0015\u0012\u0016\nwhere each \u0001\u0018\u0017 belongs to some domain or instance space \u0019 , and each label \u0007\t\u0017 is in some label\nset \u001A . For most of this paper, we assume \u001A \u001B \u001C\u001E\u001D \u001F!\u0005\f\"#\u001F%$ ; later, we discuss extensions to the\nmulticlass case. AdaBoost calls a given weak or base learning algorithm repeatedly in a series of\nrounds &'\u001B(\u001F!\u0005\u000E\r\u000E\r\u000E\r\u000F\u0005*) . One of the main ideas of the algorithm is to maintain a distribution or set of\nweights over the training set. The weight of this distribution on training example + on round & is\ndenoted ,.-/\u0000\u0011+0\n . Initially, all weights are set equally, but on each round, the weights of incorrectly\nclassiﬁed examples are increased so that the weak learner is forced to focus on the hard examples\nin the training set.\nThe weak learner’s job is to ﬁnd a weak hypothesis 12-435\u0019 6 \u001C\u001E\u001D7\u001F%\u0005\f\"#\u001F%$ appropriate for the\ndistribution ,8- . The goodness of a weak hypothesis is measured by its error\n9\n-:\u001B<;>=\u000B\u0017@?\u0013A2B\u0018CD1E-\u000B\u0000\u0002\u0001\u0013\u0017\u0011\nGF \u001BH\u0007\u0015\u0017JIK\u001B L\n\u0017JM N\fBPORQ\fSUTWV\nXZY\nS\n,#-\u000B\u0000\u0011+0\n\u0006\r\nNotice that the error is measured with respect to the distribution ,[- on which the weak learner\nwas trained. In practice, the weak learner may be an algorithm that can use the weights ,\\- on the\ntraining examples. Alternatively, when this is not possible, a subset of the training examples can\nbe sampled according to ,8- , and these (unweighted) resampled examples can be used to train the\nweak learner.\nRelating back to the horse-racing example, the instances \u0001:\u0017 correspond to descriptions of horse\nraces (such as which horses are running, what are the odds, the track records of each horse, etc.)\n2"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T14:31:54.320503Z",
     "start_time": "2025-10-05T14:31:52.788291Z"
    }
   },
   "source": [
    "embeddings = OpenAIEmbeddings() # EMBED\n",
    "embeddings\n",
    "vectordb = Chroma.from_documents(pdf_docs, embedding=embeddings) # STORE\n",
    "\n",
    "\n",
    "# Definition of a [retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/#:~:text=A%20retriever%20is,Document's%20as%20output.):\n",
    "\n",
    "# > A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. Vector stores can be used as the backbone of a retriever, but there are other types of retrievers as well.\n",
    "retriever = vectordb.as_retriever() \n",
    "# retriever\n",
    "llm = ChatOpenAI(model=MODEL, temperature=0)\n",
    "# source: https://python.langchain.com/v0.2/docs/tutorials/pdf_qa/#question-answering-with-rag\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# prompt\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T14:32:00.755881Z",
     "start_time": "2025-10-05T14:31:56.790515Z"
    }
   },
   "source": [
    "# question_answer_chain\n",
    "# This method `create_stuff_documents_chain` [outputs an LCEL runnable](https://arc.net/l/quote/bnsztwth)\n",
    "query = \"What are the key components of the transformer architecture?\"\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "results = rag_chain.invoke({\"input\": query})\n",
    "\n",
    "results"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What are the key components of the transformer architecture?',\n",
       " 'context': [Document(metadata={'moddate': '2025-10-05T12:35:19+00:00', 'creator': 'PyPDF', 'source': 'C:\\\\Users\\\\gyanr\\\\Downloads\\\\pdf_langchain_test.pdf', 'creationdate': '', 'page_label': '3', 'producer': 'Pdftools SDK', 'total_pages': 4, 'page': 2}, page_content='in the margin values. This seems to be especially so when there are only a few relevant\\nvariables so that \\x07 can be very sparse. For instance, suppose the weak hypotheses all have\\nrange \\x1c\\x1e\\x1d \\x1f!\\x05\\x0c\"#\\x1f%$ and that the label \\x07 on all examples can be computed by a majority vote of\\n\\x00 \\nof the weak hypotheses. In this case, it can be shown that if the number of relevant weak\\nhypotheses\\n\\x00 \\nis a small fraction of the total number of weak hypotheses then the margin\\nassociated with AdaBoost will be much larger than the one associated with support vector\\nmachines.\\n\\x05\\nThe computation requirements are different. The computation involved in maximizing\\nthe margin is mathematical programming, i.e., maximizing a mathematical expression given\\na set of inequalities. The difference between the two methods in this regard is that SVM cor-\\nresponds to quadratic programming, while AdaBoost corresponds only to linear program-\\nming. (In fact, as noted above, there is a deep relationship between AdaBoost and linear\\nprogramming which also connects AdaBoost with game theory and online learning [22].)\\n\\x05\\nA different approach is used to search efﬁciently in high dimensional space. Quadratic\\nprogramming is more computationally demanding than linear programming. However, there\\nis a much more important computational difference between SVM and boosting algorithms.\\nPart of the reason for the effectiveness of SVM and AdaBoost is that they ﬁnd linear classi-\\nﬁers for extremely high dimensional spaces, sometimes spaces of inﬁnite dimension. While\\nthe problem of overﬁtting is addressed by maximizing the margin, the computational prob-\\nlem associated with operating in high dimensional spaces remains. Support vector machines\\ndeal with this problem through the method of kernels which allow algorithms to perform\\nlow dimensional calculations that are mathematically equivalent to inner products in a high\\ndimensional “virtual” space. The boosting approach is instead to employ greedy search:\\nfrom this perspective, the weak learner is an oracle for ﬁnding coordinates of\\n\\x00\\n\\x00\\x11\\x01K\\n that have\\na non-negligible correlation with the label \\x07 . The reweighting of the examples changes the\\ndistribution with respect to which the correlation is measured, thus guiding the weak learner\\nto ﬁnd different correlated coordinates. Most of the actual work involved in applying SVM\\nor AdaBoost to speciﬁc classiﬁcation problems has to do with selecting the appropriate ker-\\nnel function in the one case and weak learning algorithm in the other. As kernels and weak\\nlearning algorithms are very different, the resulting learning algorithms usually operate in\\nvery different spaces and the classiﬁers that they generate are extremely different.\\nMulticlass classiﬁcation\\nSo far, we have only considered binary classiﬁcation problems in which the goal is to distinguish\\nbetween only two possible classes. Many (perhaps most) real-world learning problems, however,\\nare multiclass with more than two possible classes. There are several methods of extending Ada-\\nBoost to the multiclass case.\\nThe most straightforward generalization [23], called AdaBoost.M1, is adequate when the weak\\nlearner is strong enough to achieve reasonably high accuracy, even on the hard distributions created\\nby AdaBoost. However, this method fails if the weak learner cannot achieve at least 50% accuracy\\nwhen run on these hard distributions.\\n8'),\n",
       "  Document(metadata={'creationdate': '', 'total_pages': 4, 'source': 'C:\\\\Users\\\\gyanr\\\\Downloads\\\\pdf_langchain_test.pdf', 'page': 2, 'producer': 'Pdftools SDK', 'page_label': '3', 'creator': 'PyPDF', 'moddate': '2025-10-05T12:35:19+00:00'}, page_content='in the margin values. This seems to be especially so when there are only a few relevant\\nvariables so that \\x07 can be very sparse. For instance, suppose the weak hypotheses all have\\nrange \\x1c\\x1e\\x1d \\x1f!\\x05\\x0c\"#\\x1f%$ and that the label \\x07 on all examples can be computed by a majority vote of\\n\\x00 \\nof the weak hypotheses. In this case, it can be shown that if the number of relevant weak\\nhypotheses\\n\\x00 \\nis a small fraction of the total number of weak hypotheses then the margin\\nassociated with AdaBoost will be much larger than the one associated with support vector\\nmachines.\\n\\x05\\nThe computation requirements are different. The computation involved in maximizing\\nthe margin is mathematical programming, i.e., maximizing a mathematical expression given\\na set of inequalities. The difference between the two methods in this regard is that SVM cor-\\nresponds to quadratic programming, while AdaBoost corresponds only to linear program-\\nming. (In fact, as noted above, there is a deep relationship between AdaBoost and linear\\nprogramming which also connects AdaBoost with game theory and online learning [22].)\\n\\x05\\nA different approach is used to search efﬁciently in high dimensional space. Quadratic\\nprogramming is more computationally demanding than linear programming. However, there\\nis a much more important computational difference between SVM and boosting algorithms.\\nPart of the reason for the effectiveness of SVM and AdaBoost is that they ﬁnd linear classi-\\nﬁers for extremely high dimensional spaces, sometimes spaces of inﬁnite dimension. While\\nthe problem of overﬁtting is addressed by maximizing the margin, the computational prob-\\nlem associated with operating in high dimensional spaces remains. Support vector machines\\ndeal with this problem through the method of kernels which allow algorithms to perform\\nlow dimensional calculations that are mathematically equivalent to inner products in a high\\ndimensional “virtual” space. The boosting approach is instead to employ greedy search:\\nfrom this perspective, the weak learner is an oracle for ﬁnding coordinates of\\n\\x00\\n\\x00\\x11\\x01K\\n that have\\na non-negligible correlation with the label \\x07 . The reweighting of the examples changes the\\ndistribution with respect to which the correlation is measured, thus guiding the weak learner\\nto ﬁnd different correlated coordinates. Most of the actual work involved in applying SVM\\nor AdaBoost to speciﬁc classiﬁcation problems has to do with selecting the appropriate ker-\\nnel function in the one case and weak learning algorithm in the other. As kernels and weak\\nlearning algorithms are very different, the resulting learning algorithms usually operate in\\nvery different spaces and the classiﬁers that they generate are extremely different.\\nMulticlass classiﬁcation\\nSo far, we have only considered binary classiﬁcation problems in which the goal is to distinguish\\nbetween only two possible classes. Many (perhaps most) real-world learning problems, however,\\nare multiclass with more than two possible classes. There are several methods of extending Ada-\\nBoost to the multiclass case.\\nThe most straightforward generalization [23], called AdaBoost.M1, is adequate when the weak\\nlearner is strong enough to achieve reasonably high accuracy, even on the hard distributions created\\nby AdaBoost. However, this method fails if the weak learner cannot achieve at least 50% accuracy\\nwhen run on these hard distributions.\\n8'),\n",
       "  Document(metadata={'page_label': '1', 'source': 'C:\\\\Users\\\\gyanr\\\\Downloads\\\\pdf_langchain_test.pdf', 'moddate': '2025-10-05T12:35:19+00:00', 'creator': 'PyPDF', 'producer': 'Pdftools SDK', 'total_pages': 4, 'page': 0, 'creationdate': ''}, page_content='that suggested above. This short paper overviews some of the recent work on boosting, focusing\\nespecially on the AdaBoost algorithm which has undergone intense theoretical study and empirical\\ntesting. After introducing AdaBoost, we describe some of the basic underlying theory of boosting,\\nincluding an explanation of why it often tends not to overﬁt. We also describe some experiments\\nand applications using boosting.\\nBackground\\nBoosting has its roots in a theoretical framework for studying machine learning called the “PAC”\\nlearning model, due to Valiant [46]; see Kearns and Vazirani [32] for a good introduction to this\\nmodel. Kearns and Valiant [30, 31] were the ﬁrst to pose the question of whether a “weak” learn-\\ning algorithm which performs just slightly better than random guessing in the PAC model can be\\n“boosted” into an arbitrarily accurate “strong” learning algorithm. Schapire [38] came up with the\\nﬁrst provable polynomial-time boosting algorithm in 1989. A year later, Freund [17] developed\\na much more efﬁcient boosting algorithm which, although optimal in a certain sense, neverthe-\\nless suffered from certain practical drawbacks. The ﬁrst experiments with these early boosting\\nalgorithms were carried out by Drucker, Schapire and Simard [16] on an OCR task.\\nAdaBoost\\nThe AdaBoost algorithm, introduced in 1995 by Freund and Schapire [23], solved many of the\\npractical difﬁculties of the earlier boosting algorithms, and is the focus of this paper. Pseudocode\\nfor AdaBoost is given in Fig. 1. The algorithm takes as input a training set \\x00\\x02\\x01\\x04\\x03\\x06\\x05\\x08\\x07\\t\\x03\\x0b\\n\\x0c\\x05\\x0e\\r\\x0e\\r\\x0e\\r\\x0f\\x05\\x10\\x00\\x11\\x01\\x13\\x12\\x14\\x05\\x08\\x07\\x15\\x12\\x16\\nwhere each \\x01\\x18\\x17 belongs to some domain or instance space \\x19 , and each label \\x07\\t\\x17 is in some label\\nset \\x1a . For most of this paper, we assume \\x1a \\x1b \\x1c\\x1e\\x1d \\x1f!\\x05\\x0c\"#\\x1f%$ ; later, we discuss extensions to the\\nmulticlass case. AdaBoost calls a given weak or base learning algorithm repeatedly in a series of\\nrounds &\\'\\x1b(\\x1f!\\x05\\x0e\\r\\x0e\\r\\x0e\\r\\x0f\\x05*) . One of the main ideas of the algorithm is to maintain a distribution or set of\\nweights over the training set. The weight of this distribution on training example + on round & is\\ndenoted ,.-/\\x00\\x11+0\\n . Initially, all weights are set equally, but on each round, the weights of incorrectly\\nclassiﬁed examples are increased so that the weak learner is forced to focus on the hard examples\\nin the training set.\\nThe weak learner’s job is to ﬁnd a weak hypothesis 12-435\\x19 6 \\x1c\\x1e\\x1d7\\x1f%\\x05\\x0c\"#\\x1f%$ appropriate for the\\ndistribution ,8- . The goodness of a weak hypothesis is measured by its error\\n9\\n-:\\x1b<;>=\\x0b\\x17@?\\x13A2B\\x18CD1E-\\x0b\\x00\\x02\\x01\\x13\\x17\\x11\\nGF \\x1bH\\x07\\x15\\x17JIK\\x1b L\\n\\x17JM N\\x0cBPORQ\\x0cSUTWV\\nXZY\\nS\\n,#-\\x0b\\x00\\x11+0\\n\\x06\\r\\nNotice that the error is measured with respect to the distribution ,[- on which the weak learner\\nwas trained. In practice, the weak learner may be an algorithm that can use the weights ,\\\\- on the\\ntraining examples. Alternatively, when this is not possible, a subset of the training examples can\\nbe sampled according to ,8- , and these (unweighted) resampled examples can be used to train the\\nweak learner.\\nRelating back to the horse-racing example, the instances \\x01:\\x17 correspond to descriptions of horse\\nraces (such as which horses are running, what are the odds, the track records of each horse, etc.)\\n2'),\n",
       "  Document(metadata={'source': 'C:\\\\Users\\\\gyanr\\\\Downloads\\\\pdf_langchain_test.pdf', 'page_label': '1', 'total_pages': 4, 'moddate': '2025-10-05T12:35:19+00:00', 'creator': 'PyPDF', 'producer': 'Pdftools SDK', 'page': 0, 'creationdate': ''}, page_content='that suggested above. This short paper overviews some of the recent work on boosting, focusing\\nespecially on the AdaBoost algorithm which has undergone intense theoretical study and empirical\\ntesting. After introducing AdaBoost, we describe some of the basic underlying theory of boosting,\\nincluding an explanation of why it often tends not to overﬁt. We also describe some experiments\\nand applications using boosting.\\nBackground\\nBoosting has its roots in a theoretical framework for studying machine learning called the “PAC”\\nlearning model, due to Valiant [46]; see Kearns and Vazirani [32] for a good introduction to this\\nmodel. Kearns and Valiant [30, 31] were the ﬁrst to pose the question of whether a “weak” learn-\\ning algorithm which performs just slightly better than random guessing in the PAC model can be\\n“boosted” into an arbitrarily accurate “strong” learning algorithm. Schapire [38] came up with the\\nﬁrst provable polynomial-time boosting algorithm in 1989. A year later, Freund [17] developed\\na much more efﬁcient boosting algorithm which, although optimal in a certain sense, neverthe-\\nless suffered from certain practical drawbacks. The ﬁrst experiments with these early boosting\\nalgorithms were carried out by Drucker, Schapire and Simard [16] on an OCR task.\\nAdaBoost\\nThe AdaBoost algorithm, introduced in 1995 by Freund and Schapire [23], solved many of the\\npractical difﬁculties of the earlier boosting algorithms, and is the focus of this paper. Pseudocode\\nfor AdaBoost is given in Fig. 1. The algorithm takes as input a training set \\x00\\x02\\x01\\x04\\x03\\x06\\x05\\x08\\x07\\t\\x03\\x0b\\n\\x0c\\x05\\x0e\\r\\x0e\\r\\x0e\\r\\x0f\\x05\\x10\\x00\\x11\\x01\\x13\\x12\\x14\\x05\\x08\\x07\\x15\\x12\\x16\\nwhere each \\x01\\x18\\x17 belongs to some domain or instance space \\x19 , and each label \\x07\\t\\x17 is in some label\\nset \\x1a . For most of this paper, we assume \\x1a \\x1b \\x1c\\x1e\\x1d \\x1f!\\x05\\x0c\"#\\x1f%$ ; later, we discuss extensions to the\\nmulticlass case. AdaBoost calls a given weak or base learning algorithm repeatedly in a series of\\nrounds &\\'\\x1b(\\x1f!\\x05\\x0e\\r\\x0e\\r\\x0e\\r\\x0f\\x05*) . One of the main ideas of the algorithm is to maintain a distribution or set of\\nweights over the training set. The weight of this distribution on training example + on round & is\\ndenoted ,.-/\\x00\\x11+0\\n . Initially, all weights are set equally, but on each round, the weights of incorrectly\\nclassiﬁed examples are increased so that the weak learner is forced to focus on the hard examples\\nin the training set.\\nThe weak learner’s job is to ﬁnd a weak hypothesis 12-435\\x19 6 \\x1c\\x1e\\x1d7\\x1f%\\x05\\x0c\"#\\x1f%$ appropriate for the\\ndistribution ,8- . The goodness of a weak hypothesis is measured by its error\\n9\\n-:\\x1b<;>=\\x0b\\x17@?\\x13A2B\\x18CD1E-\\x0b\\x00\\x02\\x01\\x13\\x17\\x11\\nGF \\x1bH\\x07\\x15\\x17JIK\\x1b L\\n\\x17JM N\\x0cBPORQ\\x0cSUTWV\\nXZY\\nS\\n,#-\\x0b\\x00\\x11+0\\n\\x06\\r\\nNotice that the error is measured with respect to the distribution ,[- on which the weak learner\\nwas trained. In practice, the weak learner may be an algorithm that can use the weights ,\\\\- on the\\ntraining examples. Alternatively, when this is not possible, a subset of the training examples can\\nbe sampled according to ,8- , and these (unweighted) resampled examples can be used to train the\\nweak learner.\\nRelating back to the horse-racing example, the instances \\x01:\\x17 correspond to descriptions of horse\\nraces (such as which horses are running, what are the odds, the track records of each horse, etc.)\\n2')],\n",
       " 'answer': 'The key components of the transformer architecture include the encoder and decoder layers, which consist of multi-head self-attention mechanisms and feed-forward neural networks. Additionally, transformers utilize positional encoding to retain the order of input sequences, and layer normalization is applied to stabilize training. The architecture is designed to process sequences in parallel, making it efficient for tasks like natural language processing.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T14:32:15.540514Z",
     "start_time": "2025-10-05T14:32:15.535999Z"
    }
   },
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "final_answer = results[\"answer\"]\n",
    "\n",
    "Markdown(final_answer)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "The key components of the transformer architecture include the encoder and decoder layers, which consist of multi-head self-attention mechanisms and feed-forward neural networks. Additionally, transformers utilize positional encoding to retain the order of input sequences, and layer normalization is applied to stabilize training. The architecture is designed to process sequences in parallel, making it efficient for tasks like natural language processing."
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T14:32:55.571096Z",
     "start_time": "2025-10-05T14:32:50.499046Z"
    }
   },
   "source": [
    "query_summary = \"Write a simple bullet points summary about this paper\"\n",
    "\n",
    " # adding chat history so the model remembers previous questions\n",
    "output = rag_chain.invoke({\"input\": query_summary})\n",
    "\n",
    "Markdown(output[\"answer\"])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "- The paper focuses on the AdaBoost algorithm, a prominent boosting method in machine learning.\n- It discusses the theoretical foundations of boosting, particularly the PAC learning model, and the evolution of boosting algorithms.\n- AdaBoost improves upon earlier algorithms by effectively managing weights on training examples, enhancing performance on difficult cases.\n- The algorithm is fast, simple, and requires no prior knowledge about the weak learner, making it versatile for various applications.\n- Empirical tests show AdaBoost's effectiveness in tasks like OCR, text categorization, and natural language processing, often outperforming other methods."
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T14:33:02.469204Z",
     "start_time": "2025-10-05T14:32:59.477092Z"
    }
   },
   "source": [
    "def ask_pdf(pdf_qa,query):\n",
    "    print(\"QUERY: \",query)\n",
    "    result = pdf_qa.invoke({\"input\": query})\n",
    "    answer = result[\"answer\"]\n",
    "    print(\"ANSWER\", answer)\n",
    "    return answer\n",
    "\n",
    "\n",
    "ask_pdf(rag_chain,\"How does the self-attention mechanism in transformers differ from traditional sequence alignment methods?\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY:  How does the self-attention mechanism in transformers differ from traditional sequence alignment methods?\n",
      "ANSWER The self-attention mechanism in transformers allows each element in a sequence to attend to all other elements, enabling the model to capture long-range dependencies and contextual relationships effectively. In contrast, traditional sequence alignment methods typically focus on pairwise comparisons, aligning sequences based on local similarities without considering the broader context of the entire sequence. This results in transformers being more flexible and powerful for tasks involving complex relationships in sequential data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The self-attention mechanism in transformers allows each element in a sequence to attend to all other elements, enabling the model to capture long-range dependencies and contextual relationships effectively. In contrast, traditional sequence alignment methods typically focus on pairwise comparisons, aligning sequences based on local similarities without considering the broader context of the entire sequence. This results in transformers being more flexible and powerful for tasks involving complex relationships in sequential data.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T14:33:07.673877Z",
     "start_time": "2025-10-05T14:33:05.860763Z"
    }
   },
   "source": [
    "quiz_questions = ask_pdf(rag_chain, \"Quiz me with 3 simple questions on the positional encodings and the role they play in transformers.\")\n",
    "\n",
    "quiz_questions"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY:  Quiz me with 3 simple questions on the positional encodings and the role they play in transformers.\n",
      "ANSWER 1. What is the primary purpose of positional encodings in transformer models?  \n",
      "2. How do positional encodings help transformers understand the order of input sequences?  \n",
      "3. What are two common methods for implementing positional encodings in transformers?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1. What is the primary purpose of positional encodings in transformer models?  \\n2. How do positional encodings help transformers understand the order of input sequences?  \\n3. What are two common methods for implementing positional encodings in transformers?'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T14:33:07.680406Z",
     "start_time": "2025-10-05T14:33:07.676890Z"
    }
   },
   "source": [
    "llm = ChatOpenAI(model=MODEL, temperature=0.0)"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T14:33:09.713236Z",
     "start_time": "2025-10-05T14:33:09.702158Z"
    }
   },
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class QA(BaseModel):\n",
    "    questions: List[str] = Field(description='List of questions about a given context.')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3577: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T14:33:12.463529Z",
     "start_time": "2025-10-05T14:33:12.459173Z"
    }
   },
   "source": [
    "from langchain_core.prompts.chat import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "template = f\"You transform unstructured questions about a topic into a structured list of questions.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(\"Return ONLY a PYTHON list containing the questions in this text: {questions}\")"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T14:33:15.061386Z",
     "start_time": "2025-10-05T14:33:15.056992Z"
    }
   },
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt,human_message_prompt])"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T14:33:16.308751Z",
     "start_time": "2025-10-05T14:33:16.302170Z"
    }
   },
   "source": [
    "quiz_chain = chat_prompt | llm.with_structured_output(QA)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gyanr\\gyan-python-workspace\\jup-workspace\\venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1900: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T14:33:20.090278Z",
     "start_time": "2025-10-05T14:33:18.048182Z"
    }
   },
   "source": [
    "questions_list = quiz_chain.invoke({\"questions\": quiz_questions})\n",
    "questions_list"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QA(questions=['What is the primary purpose of positional encodings in transformer models?', 'How do positional encodings help transformers understand the order of input sequences?', 'What are two common methods for implementing positional encodings in transformers?'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T14:33:23.500313Z",
     "start_time": "2025-10-05T14:33:23.494765Z"
    }
   },
   "source": [
    "questions = questions_list.questions"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T14:33:24.538826Z",
     "start_time": "2025-10-05T14:33:24.532314Z"
    }
   },
   "source": [
    "questions"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the primary purpose of positional encodings in transformer models?',\n",
       " 'How do positional encodings help transformers understand the order of input sequences?',\n",
       " 'What are two common methods for implementing positional encodings in transformers?']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T14:33:34.286089Z",
     "start_time": "2025-10-05T14:33:25.919106Z"
    }
   },
   "source": [
    "# the questions variable was created within the string inside the `questions_list` variable.\n",
    "answers = []\n",
    "for q in questions:\n",
    "    answers.append(ask_pdf(rag_chain,q))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY:  What is the primary purpose of positional encodings in transformer models?\n",
      "ANSWER The primary purpose of positional encodings in transformer models is to provide information about the order of the input tokens, as transformers do not have a built-in mechanism to capture sequence order due to their parallel processing nature. Positional encodings allow the model to differentiate between tokens based on their positions in the sequence, enabling it to understand the relationships and context of the tokens effectively. This is crucial for tasks involving sequential data, such as natural language processing.\n",
      "QUERY:  How do positional encodings help transformers understand the order of input sequences?\n",
      "ANSWER Positional encodings provide information about the position of each token in the input sequence, allowing transformers to capture the order of the tokens since they lack inherent sequential processing. By adding these encodings to the input embeddings, transformers can differentiate between tokens based on their positions, enabling them to understand the relationships and context within the sequence. This mechanism is crucial for tasks where the order of words significantly impacts meaning, such as in natural language processing.\n",
      "QUERY:  What are two common methods for implementing positional encodings in transformers?\n",
      "ANSWER Two common methods for implementing positional encodings in transformers are sinusoidal positional encodings and learned positional encodings. Sinusoidal encodings use sine and cosine functions of different frequencies to create a fixed encoding for each position, while learned encodings involve training a set of positional embeddings that are optimized during the training process. Both methods help the model understand the order of the input sequence.\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T14:33:36.503290Z",
     "start_time": "2025-10-05T14:33:34.288611Z"
    }
   },
   "source": [
    "evaluations = []\n",
    "\n",
    "for q,a in zip(questions, answers):\n",
    "    # Check for results\n",
    "    evaluations.append(ask_pdf(rag_chain,f\"Is this: {a} the correct answer to this question: {q} according to the paper? Return ONLY '''YES''' or '''NO'''. Output:\"))\n",
    "\n",
    "evaluations"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY:  Is this: The primary purpose of positional encodings in transformer models is to provide information about the order of the input tokens, as transformers do not have a built-in mechanism to capture sequence order due to their parallel processing nature. Positional encodings allow the model to differentiate between tokens based on their positions in the sequence, enabling it to understand the relationships and context of the tokens effectively. This is crucial for tasks involving sequential data, such as natural language processing. the correct answer to this question: What is the primary purpose of positional encodings in transformer models? according to the paper? Return ONLY '''YES''' or '''NO'''. Output:\n",
      "ANSWER YES\n",
      "QUERY:  Is this: Positional encodings provide information about the position of each token in the input sequence, allowing transformers to capture the order of the tokens since they lack inherent sequential processing. By adding these encodings to the input embeddings, transformers can differentiate between tokens based on their positions, enabling them to understand the relationships and context within the sequence. This mechanism is crucial for tasks where the order of words significantly impacts meaning, such as in natural language processing. the correct answer to this question: How do positional encodings help transformers understand the order of input sequences? according to the paper? Return ONLY '''YES''' or '''NO'''. Output:\n",
      "ANSWER YES\n",
      "QUERY:  Is this: Two common methods for implementing positional encodings in transformers are sinusoidal positional encodings and learned positional encodings. Sinusoidal encodings use sine and cosine functions of different frequencies to create a fixed encoding for each position, while learned encodings involve training a set of positional embeddings that are optimized during the training process. Both methods help the model understand the order of the input sequence. the correct answer to this question: What are two common methods for implementing positional encodings in transformers? according to the paper? Return ONLY '''YES''' or '''NO'''. Output:\n",
      "ANSWER NO\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['YES', 'YES', 'NO']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T14:33:38.862871Z",
     "start_time": "2025-10-05T14:33:38.858791Z"
    }
   },
   "source": [
    "scores = []\n",
    "\n",
    "yes_count = evaluations.count('YES')\n",
    "score = str(yes_count/len(evaluations) * 100) + \"%\"\n",
    "print(score)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.66666666666666%\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more 'langchain way' to do this would be:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T14:33:46.019670Z",
     "start_time": "2025-10-05T14:33:46.008125Z"
    }
   },
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "prompt_eval_template = \"\"\"\n",
    "You take in context, a question and a generated answer and you output ONLY a score of YES if the answer is correct,\n",
    "or NO if the answer is not correct.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "<context>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "<question>\n",
    "\n",
    "<answer>\n",
    "{answer}\n",
    "<answer>\n",
    "\"\"\"\n",
    "\n",
    "prompt_eval = ChatPromptTemplate.from_template(prompt_eval_template)\n",
    "\n",
    "answer_eval_chain = (\n",
    "    {\n",
    "        'context': lambda x: format_docs(x['context']),\n",
    "        'question': lambda x: x['question'],\n",
    "        'answer': lambda x: x['answer']\n",
    "        }\n",
    "    ) | prompt_eval | llm | StrOutputParser()"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T14:33:50.519342Z",
     "start_time": "2025-10-05T14:33:48.814143Z"
    }
   },
   "source": [
    "evaluations = []\n",
    "for q,a in zip(questions, answers):\n",
    "    evaluations.append(answer_eval_chain.invoke({'context': pdf_docs, 'question': q, 'answer': a}))"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T14:33:52.398239Z",
     "start_time": "2025-10-05T14:33:52.393737Z"
    }
   },
   "source": [
    "evaluations"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['YES', 'YES', 'YES']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T14:33:54.390954Z",
     "start_time": "2025-10-05T14:33:54.387224Z"
    }
   },
   "source": [
    "scores = []\n",
    "\n",
    "yes_count = evaluations.count('YES')\n",
    "score = str(yes_count/len(evaluations) * 100) + \"%\"\n",
    "print(score)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example notebook we introduced a few interesting ideas:\n",
    "1. Structured outputs\n",
    "2. Some simple evaluation of rag answers using the 'llm-as-a-judge' strategy"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
